[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MIRS Lab guides",
    "section": "",
    "text": "Preface\nThis document is a series of guides to help students that are working with infrared spectroscopy in the context of vector surveillance. Here, there are information about to the use of our scripts to extract data, clean it, assembling data sets for machine/deep learning as well as tutorials on how to plot data, machine/deep learning pipelines.\nThis guide is based on the following published work:\n\nGonzález Jiménez, Mario, et al. “Prediction of mosquito species and population age structure using mid-infrared spectroscopy and supervised machine learning.” Wellcome open research 4 (2019).(González Jiménez et al. 2019)\nSiria, Doreen J., et al. “Rapid age-grading and species identification of natural mosquitoes for malaria surveillance.” Nature Communications 13.1 (2022): 1501. (Siria et al. 2022).\nMwanga, E.P., Siria, D.J., Mitton, J. et al. Using transfer learning and dimensionality reduction techniques to improve generalisability of machine-learning predictions of mosquito ages from mid-infrared spectra. BMC Bioinformatics 24, 11 (2023). (Mwanga et al. 2023)\nPazmiño-Betancourth, Mauro, et al. “Evaluation of diffuse reflectance spectroscopy for predicting age, species, and cuticular resistance of Anopheles gambiae sl under laboratory conditions.” Scientific Reports 13.1 (2023): 18499.(Pazmiño-Betancourth et al. 2023)\nPazmiño-Betancourth, Mauro, et al. “Advancing age grading techniques for Glossina morsitans morsitans, vectors of African trypanosomiasis, through mid-infrared spectroscopy and machine learning.” Biology Methods and Protocols 9.1 (2024): bpae058. (Pazmiño-Betancourth et al. 2024)\n\nif you have any questions you can reach me or Ivan Casas\n\n\n\n\nGonzález Jiménez, Mario, Simon A. Babayan, Pegah Khazaeli, Margaret Doyle, Finlay Walton, Elliott Reedy, Thomas Glew, et al. 2019. “Prediction of Mosquito Species and Population Age Structure Using Mid-Infrared Spectroscopy and Supervised Machine Learning [Version 3; Peer Review: 2 Approved].” Wellcome Open Research. https://doi.org/10.12688/wellcomeopenres.15201.3.\n\n\nMwanga, Emmanuel P., Doreen J. Siria, Joshua Mitton, Issa H. Mshani, Mario González-Jiménez, Prashanth Selvaraj, Klaas Wynne, Francesco Baldini, Fredros O. Okumu, and Simon A. Babayan. 2023. “Using Transfer Learning and Dimensionality Reduction Techniques to Improve Generalisability of Machine-Learning Predictions of Mosquito Ages from Mid-Infrared Spectra.” BMC Bioinformatics 24 (1): 11. https://doi.org/10.1186/s12859-022-05128-5.\n\n\nPazmiño-Betancourth, Mauro, Ivan Casas Gómez-Uribarri, Karina Mondragon-Shem, Simon A Babayan, Francesco Baldini, and Lee Rafuse Haines. 2024. “Advancing Age Grading Techniques for Glossina Morsitans Morsitans, Vectors of African Trypanosomiasis, Through Mid-Infrared Spectroscopy and Machine Learning.” Biology Methods and Protocols, August, bpae058. https://doi.org/10.1093/biomethods/bpae058.\n\n\nPazmiño-Betancourth, Mauro, Victor Ochoa-Gutiérrez, Heather M. Ferguson, Mario González-Jiménez, Klaas Wynne, Francesco Baldini, and David Childs. 2023. “Evaluation of Diffuse Reflectance Spectroscopy for Predicting Age, Species, and Cuticular Resistance of Anopheles Gambiae s.l Under Laboratory Conditions.” Scientific Reports 13 (1): 18499. https://doi.org/10.1038/s41598-023-45696-x.\n\n\nSiria, Doreen J., Roger Sanou, Joshua Mitton, Emmanuel P. Mwanga, Abdoulaye Niang, Issiaka Sare, Paul C. D. Johnson, et al. 2022. “Rapid Age-Grading and Species Identification of Natural Mosquitoes for Malaria Surveillance.” Nature Communications 13 (1): 1–9. https://doi.org/10.1038/s41467-022-28980-8."
  },
  {
    "objectID": "howtouse.html#opus-dei",
    "href": "howtouse.html#opus-dei",
    "title": "1  How to use our scripts for mid infrared spectroscopy",
    "section": "1.1 Opus dei",
    "text": "1.1 Opus dei\nThis script takes the files from the FTIR machine that are proprietary (you can only read them with the OPUS software) and transform them into .dpt (data point table) files and .mzz files. Dpt files are a two column file with one column showing wavenumbers values and the second column showing the absorbance. These type of files can be opened in Excel, notes, etc. Mzz files are type of compressed file created specifically for this project. They contain only the absorbance and they can be opened by bad_blood script."
  },
  {
    "objectID": "howtouse.html#bad-blood",
    "href": "howtouse.html#bad-blood",
    "title": "1  How to use our scripts for mid infrared spectroscopy",
    "section": "1.2 Bad blood",
    "text": "1.2 Bad blood\nThis script reads all your files, either as .dpt or .mzz, and assess the quality of each file (low intensity, atmospheric interference, and distortion by the anvil). Then, it will generate a table with all the data, where each row will be a sample and each column will be the metadata and each wavenumber value. This table can use for your machine learning analysis."
  },
  {
    "objectID": "howtouse.html#how-to-use-the-scripts",
    "href": "howtouse.html#how-to-use-the-scripts",
    "title": "1  How to use our scripts for mid infrared spectroscopy",
    "section": "1.3 How to use the scripts",
    "text": "1.3 How to use the scripts\nThe workflow is as follows: - First, run the opus dei script to decode the raw files from opus format to .dpt or .mzz - Second, run the bad blood script on the decoded files (.dpt or .mzz) to check the quality of the files and build the final dataframe.\nThere are two approaches (maybe three) on how you can use them in your workflow. These are:\n\nUsing the jupyter notebook file only\nUsing a combination of jupyter notebook and the .py files\n\n\n1.3.1 Using only jupyter notebook\nThis is self explanatory. You just need to copy the jupyter notebook into your working folder. Open it with jupyter, JupyterLab or vscode and follow the instructions.\n\n\n1.3.2 Using both a jupyter notebook and the .py files approach\nThis is the approach for intermediate/advance users. Combines the flexibility of jupyter notebooks plus the convenience of having all your functions in a separate file. This improves readability and maintainability of your code (and it looks more slick!). For this you will need to have a specific folder structure. Something like this:\nIn the folder called source code (src), you will put the bad_blood.py and opus_dei.py files. Then, you will create your jupyter notebook for your analyses. You need to include two important pieces of code.\nFirst:\n%load_ext autoreload\n%autoreload 2\nThis code needs to be run once (at the beginning of your session). This allows you to change or improve your functions from the source code folder and they will be updated automatically in the jupyter notebook.\nThe second code block you will need is this:\nsys.path.append('/your/path/toyour/sourcecode/src’)\nYou need to specify the path where bad blood and opus dei are located, so jupyter knows where to search when you import those functions. This can be added in the cell where you import all your packages.\n\n\n\n\n\n\nTip\n\n\n\nTry to use a IDE such as Visual Studio Code from the beginning."
  },
  {
    "objectID": "howtouse.html#examples-on-how-to-use-the-opus_dei.py-and-bad_blood.py",
    "href": "howtouse.html#examples-on-how-to-use-the-opus_dei.py-and-bad_blood.py",
    "title": "1  How to use our scripts for mid infrared spectroscopy",
    "section": "1.4 Examples on how to use the opus_dei.py and bad_blood.py",
    "text": "1.4 Examples on how to use the opus_dei.py and bad_blood.py\nTo run opus dei and bad blood you need the following packages: numpy and pandas.\nYou need to set up your jupyter notebook (this also work if you are only with python scripts)\n\n# setting up the path to src folder\nimport sys\nsys.path.append(\"/Users/mauropazmino/Documents/University/Workhops/How to use the scripts/src\")\n\n# the packages that you will use\nimport numpy \nimport pandas\n\n# importing opus dei and bad blood functions\nfrom bad_blood import bad_blood_v3_real\nfrom opus_dei import opus_dei\nOnce you have imported your modules, you need to run opus_dei function to decode the files into readable format.\nThe opus dei function requires two parameters: the folder where the raw files are located and the folder where you want to save the new files.\n\nraw_data_folder = \"./data/raw_data\"\n\nprocessed_data_folder = \"./data/processed_data\"\n\n# run the function\nopus_dei(raw_data_folder, processed_data_folder)\nOnce the files has been decoded, you will get a message.\nAfter that, you can run the bad blood function with the processed files.\nThe bad blood function requires two parameters, one is the path where the processed files are located and two, the name of the assembled datafile. Moreover, this function requires all the files have the same file length. In our lab, we have the following filename convention:\n\nXX-XX-XX-XX-XX.dpt\n\nwhere each part of the file that is separate by a hyphen represents some metadata.For example in the following file:\nUV-AG-F-NOU-R1.0.dpt\nUV means UV experiment, AG means Anopheles gambiae, F means female, etc. It does not matter how many sections you have in your file name (2 from 100), as long as all your files have the same number of sections. If one file has less or more, the function will give you an error and it will indicate which files need to be fixed. You can do that on the .mzz files but it would be better if you do it in your raw files and then run opus dei again.\n\n\n\n\n\n\nFile name length is important!\n\n\n\nAlways check that your files have the same length.\n\n\nNow you run the function bad_blood\nprocessed_data_folder = \"./data/processed_data\"\nname_data = 'lab_guide_sample'\n\nbad_blood_v3_real(processed_data_folder,name_data)\nAs you can see, bad blood will read all your files and will do a quality control on each one of them. If a file does not pass the quality control, it will be discarded. After that, it will put all you files together with the metadata tied to them in a nice table format. The name of the final data set will include the date when it was created for future reference."
  },
  {
    "objectID": "howtouse.html#errors-using-bad-blood",
    "href": "howtouse.html#errors-using-bad-blood",
    "title": "1  How to use our scripts for mid infrared spectroscopy",
    "section": "1.5 Errors using bad blood",
    "text": "1.5 Errors using bad blood\n\n\n\n\n\n\nErrors when using bad blood\n\n\n\nBad blood will give the following error if the folder where it reads the files does not not have any mzz or dpt files on it.\nUnboundLocalError: cannot access local variable 'tmp2' \nwhere it is not associated with a value\n\n\nIf you want to know more what bad blood does, you can check Chapter 6 for more information"
  },
  {
    "objectID": "visualization.html#basic-plot-with-defaults",
    "href": "visualization.html#basic-plot-with-defaults",
    "title": "2  Visualization of spectroscopy data",
    "section": "2.1 Basic plot with defaults",
    "text": "2.1 Basic plot with defaults\nFirst, we need to import the data that we are going to use. The data will be arranged with samples as rows and wavenumbers and categories as a columns.\n\n# import packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\ndata = pd.read_csv(\"./data/UV_pilot.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nSpecie\nSex\nExposed\nReeplicate\nID\n4006\n4004\n4002\n4000\n3998\n...\n415\n413\n411\n409\n407\n405\n403\n401\n399\n397\n\n\n\n\n0\nAG\nF\nYES\nR1\n2\n0.00477\n0.00462\n0.00448\n0.00434\n0.00439\n...\n0.33421\n0.33535\n0.33473\n0.33308\n0.33221\n0.33094\n0.33106\n0.33498\n0.34019\n0\n\n\n1\nAG\nF\nYES\nR1\n3\n0.00596\n0.00631\n0.00629\n0.00621\n0.00623\n...\n0.32344\n0.31985\n0.31923\n0.32013\n0.31983\n0.31645\n0.31177\n0.31150\n0.31581\n0\n\n\n2\nAG\nF\nYES\nR1\n8\n0.00542\n0.00513\n0.00511\n0.00517\n0.00537\n...\n0.33420\n0.33665\n0.33628\n0.33366\n0.33190\n0.33080\n0.33179\n0.33579\n0.34106\n0\n\n\n3\nAG\nF\nNOU\nR1\n1\n0.00681\n0.00690\n0.00695\n0.00683\n0.00666\n...\n0.29993\n0.30024\n0.30117\n0.30262\n0.30231\n0.30015\n0.30060\n0.30385\n0.30474\n0\n\n\n4\nAG\nF\nNOU\nR1\n3\n0.00543\n0.00552\n0.00556\n0.00555\n0.00548\n...\n0.29425\n0.29175\n0.29122\n0.29344\n0.29576\n0.29720\n0.29858\n0.29935\n0.29861\n0\n\n\n\n\n5 rows × 1759 columns\n\n\n\nWe select the wavenumber and their absorbance values and we call this “X”. We also select the categorical values we want to plot, and call it “y_labels”\n\nX = data.loc[:,\"4000\":'401']\ny_labels = data.loc[:,\"Exposed\"]\n\nTo plot a line, we need x and y values. The values of x will be the specific wavenumbers (which are the names of the columns of X). We extract the names of the columns of X and put them on a list as a integers.\n\nwnLabels = X.columns.values\nwaveNums = [int(x) for x in wnLabels]\n\nNow that we have teh values of x, we need the absorbance of a sample. We can select any row from X.\n\nfig, ax = plt.subplots(figsize=(6,3))\nax.plot(waveNums, X.iloc[2,:])"
  },
  {
    "objectID": "visualization.html#improving-the-defaults",
    "href": "visualization.html#improving-the-defaults",
    "title": "2  Visualization of spectroscopy data",
    "section": "2.2 Improving the defaults",
    "text": "2.2 Improving the defaults\nThe defaults of matplotlib or seaborn are not that great. Increasing the linewidth and font size, choosing another colour and add axis labels can help with the overall quality and readability of the plot. Moreover, infrared spectral data is plot from 4000 to 400 cm\\(^{-1}\\). So, we need to reverse the x-axis.\n\nfig, ax = plt.subplots(figsize=(6,3), tight_layout=True)\nax.plot(waveNums,X.iloc[2,:],color='r',linewidth=1.5)\n\nax.set_xlim(4000, 600)\nax.set_ylabel(\"Absorbance (a.u)\", fontweight='bold')\nax.set_xlabel('Wavenumbers (cm$^{-1}$)', fontweight='bold')\n\nText(0.5, 0, 'Wavenumbers (cm$^{-1}$)')\n\n\n\n\n\nWe can choose different aesthetics for the final plot like removing the spines.\n\nfig, ax = plt.subplots(figsize=(6,3), tight_layout=True)\nax.plot(waveNums,X.iloc[2,:],color='r',linewidth=1.5)\n\nax.set_xlim(4000, 600)\nax.set_ylabel(\"Absorbance (a.u)\", fontweight='bold')\nax.set_xlabel('Wavenumbers (cm$^{-1}$)', fontweight='bold')\nax.spines.right.set_visible(False)\nax.spines.top.set_visible(False)"
  },
  {
    "objectID": "visualization.html#plotting-the-means-of-different-classes",
    "href": "visualization.html#plotting-the-means-of-different-classes",
    "title": "2  Visualization of spectroscopy data",
    "section": "2.3 Plotting the means of different classes",
    "text": "2.3 Plotting the means of different classes\nTo observe differences between classes, you might want to plot the mean spectra of each class you are assessing. The issue here is that the data is presented as rows, which complicates the plotting process since matplotlib like the samples in columns rather than rows.\nThe code is straightforward, and it works for any number of classes (as long as the number of colours is the same of greater than the number of classes)\n\n# color hex codes can be used, as well as common color names. You can get more codes at https://www.color-hex.com/\n\ncolors = ['#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e']\n\n# figure\nfig, ax = plt.subplots(figsize=(6,3))\n\n\nfor i, c in zip(np.unique(y_labels), colors):\n    sn.lineplot(x=waveNums, y=np.mean(X[y_labels == i], axis=0), label=i, color=c)\nplt.legend()\nax.set_xlim(4000,401)\nax.set_title(\"MIR spectra\",fontsize=16)\nax.set_xlabel(\"Wavenumber (cm$^{-1}$)\",fontsize=12)\nax.set_ylabel(\"Absorbance (a.u)\",fontsize=12)\n\nText(0, 0.5, 'Absorbance (a.u)')"
  },
  {
    "objectID": "visualization.html#plotting-all-samples-of-different-classes",
    "href": "visualization.html#plotting-all-samples-of-different-classes",
    "title": "2  Visualization of spectroscopy data",
    "section": "2.4 Plotting all samples of different classes",
    "text": "2.4 Plotting all samples of different classes\nWhat if you want to plot not the means but all the samples? For this, we use the same approach as before, but instead of selecting all the samples and plot the mean, we select each sample and plot it. We use itertuples to go through them each row and plot it with the assigned colour. It might be a little bit slow if you have thousands of rows.\n\nfig, ax = plt.subplots(figsize=(6,3))\n\n# Iterates through each row and compare it to the label, and plot it. \nfor i, c in zip(np.unique(y_labels), colors):\n    for row in X.loc[y_labels == i].itertuples(index=False):\n        ax.plot(waveNums, row, color=c, label=i)\n\n# Legend creation       \nhandles, labels_2 = ax.get_legend_handles_labels()\nnewLabels, newHandles = [], []\nfor handle, label_1 in zip(handles, labels_2):\n    if label_1 not in newLabels:\n        newLabels.append(label_1)\n        newHandles.append(handle)\n\nax.legend(newHandles, newLabels)\nax.set_xlim(4000,401)\nax.set_title(\"MIR spectra\",fontsize=16)\nax.set_xlabel(\"Wavenumber (cm$^{-1}$)\",fontsize=12)\nax.set_ylabel(\"Absorbance (a.u)\",fontsize=12)\n\nText(0, 0.5, 'Absorbance (a.u)')"
  },
  {
    "objectID": "mlpipelines.html#basic-exploratory-machine-learning-analysis",
    "href": "mlpipelines.html#basic-exploratory-machine-learning-analysis",
    "title": "3  Machine learning pipelines",
    "section": "3.1 Basic exploratory machine learning analysis",
    "text": "3.1 Basic exploratory machine learning analysis\nThis is a pipeline that can be used to compare different models and decide which one is the best for your case and further optimize it and test it on our test data set.\n\n3.1.1 Explore different models\nFirst, you need to import your data, separate features (absorbances) and the target variable, and split the data set into train and test set.\n\n\n# Import data\ndf = pd.read_csv(\"../data/UV_pilot.csv\")\n\n# Split features and target\nX = df.loc[:,\"4000\":\"403\"]\ny = df.loc[:,\"Exposed\"]\n\n# Split into train and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.2)\n\nSource: basic_ml_analysis.ipynb\n\n\n\n\n\n\nImportant\n\n\n\nBe sure you stratify your split by using the stratify parameter. This is to make sure the different classes of the target variable are equality distribute in the split\n\n\nYou need to create a dictionary of the models you want to test. For this example, we are going to use 3 algorithms: Logistic regression, Support Vector Machines, and Random Forests but you can add more models if needed.\n\n\nmodel_dict = {\n    \"LR\": LogisticRegression(),\n    \"SVM\": SVC(),\n    \"RF\": RandomForestClassifier(),\n}\n\nSource: basic_ml_analysis.ipynb\nIt is good practice to use pipelines to avoid data leakage especially, if we are using preprocessing algorithms in our data. Usually, we scale the data using StandardScaler(with_std=False). So for this, we will build a pipeline that includes the preprocessing algorithm and the model. Click here if you want to learn more about pipelines and how to use them.\nThen, we create a for loop which is going to iterate between each model, do a cross validation with our specified pipeline, append the results into a list and print a message with the accuracy and the standard deviation of each model.\n\n\nresults = []\nnames = []\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=7)\n\nfor key, model in model_dict.items():\n    pipe = Pipeline([('scaler', StandardScaler(with_std=False)), ('model', model)])\n\n    cv_results = cross_val_score(pipe, X_train, y_train, cv=sss, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(key)\n    print(f'Accuracy using {key} is {cv_results.mean():.2f} ± {cv_results.std():.2f}')\n\n\nAccuracy using LR is 0.53 ± 0.08\nAccuracy using SVM is 0.45 ± 0.04\nAccuracy using RF is 0.69 ± 0.09\n\n\nSource: basic_ml_analysis.ipynb\nWe can examine our results more with a plot\n\n\nfig, ax = plt.subplots()\nax.boxplot(results)\nax.set_xticklabels(names)\n\n[Text(1, 0, 'LR'), Text(2, 0, 'SVM'), Text(3, 0, 'RF')]\n\n\n\n\n\nSource: basic_ml_analysis.ipynb\nFrom the plot, we can see that random forest has the highest accuracy. So, we choose it as our model to further optimize it.\n\n\n3.1.2 Model optimization\nModel optimization consists on finding the best combination of our model’s hyperparameter values that will give us the best accuracy (or any metric that we specify). For this, we need to know which hyperparameters can be change, what values they accept and how they affect the model.\nFor random forests, we can try different values of max_depth to control the size of the tree to prevent overfitting, min_samples_split or min_samples_leaf to ensure that multiple samples inform every decision in the tree, (very small number will make the model overfit, whereas a large number will prevent the tree from learning the data). In this example, we are going to try different values of max_depth and min_samples_split. We create a dictionary with the hyperparameters and the different values we cant to try. A good guide about hyperparameters in random forests can be found here.\n\n\n# create a dictionary with different hyperparameters and the values you want to test\n\nparam_grid = {'model__max_depth': [5, 6],\n            'model__min_samples_split': [2, 3, 4]}\n\n# create the new pipleine with the model we choose to optimize\npipe = Pipeline([('scaler', StandardScaler(with_std=False)), ('model', RandomForestClassifier(class_weight='balanced'))])\n\nSource: basic_ml_analysis.ipynb\nSklearn in-built function GridsearchCV will go over each of the possible combinations between the different values of each hyperparameter and calculate the accuracy of each combination. Then, it will choose the model with the best accuracy and the parameters that uses.\n\n\nsearch = GridSearchCV(pipe, param_grid, n_jobs=2)\nsearch.fit(X_train, y_train)\nprint(f\"Best parameter (CV score={search.best_score_:.2f})\")\nprint(search.best_params_)\n\nBest parameter (CV score=0.68)\n{'model__max_depth': 6, 'model__min_samples_split': 2}\n\n\nSource: basic_ml_analysis.ipynb\nHere, we can see that the best parameters with a cross-validation value of 0.68 are ’model__max_depth’: 6, ’model__min_samples_split’: 2, You can select the best pipeline from the object using .best_estimator_ attribute.\n\n\nsearch.best_estimator_\n\nPipeline(steps=[('scaler', StandardScaler(with_std=False)),\n                ('model',\n                 RandomForestClassifier(class_weight='balanced', max_depth=6))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('scaler', StandardScaler(with_std=False)),\n                ('model',\n                 RandomForestClassifier(class_weight='balanced', max_depth=6))])  StandardScaler?Documentation for StandardScalerStandardScaler(with_std=False)  RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier(class_weight='balanced', max_depth=6) \n\n\nSource: basic_ml_analysis.ipynb\n\n\n\n\n\n\nImportant\n\n\n\nModel optimization can be a extremely long process. Be mindful of the hyperparameters you want to test. This will require you to really study the model and make sense of the values you are going to use. Again, scikit-learn documentation is extremely helpful!\n\n\n\n\n3.1.3 Test the final model\nThe final optimized model (or in this case pipeline) is tested on the unseen data you split at the beginning of the analysis. Fit the optimized model to your train data set and calculate the predicted values\n\n\noptimized_model = search.best_estimator_\noptimized_model.fit(X_train, y_train)\ny_pred = optimized_model.predict(X_test)\n\nSource: basic_ml_analysis.ipynb\nYour final score, confusion matrix and auc-roc curve. We expect very poor perfomance since there is no really a signal between the groups exposed and not exposed to UV.\n\n\n# Accuracy and confusion  matrix\n\nfig, (ax, ax2) = plt.subplots(1,2, figsize=(10,4), tight_layout=True)\n\n\nConfusionMatrixDisplay.from_predictions(y_test, y_pred, normalize='true', cmap='Reds',ax=ax)\n\nRocCurveDisplay.from_estimator(optimized_model, X_test, y_test, ax=ax2)\n\n\ntitle1 = (f'Accuracy of final model = {accuracy_score(y_test, y_pred):.3f}')\nax.set_title(title1)\n\nText(0.5, 1.0, 'Accuracy of final model = 0.750')\n\n\n\n\n\nSource: basic_ml_analysis.ipynb\n\n\n3.1.4 How can we check what features are important?\nThis is a hot topic, at least for me. For various reasons: correlation of the features, the model that you use, misleading results, etc. A way to have the feeling about what the model is learning is to look at the coefficients. Some models will have feature importance as an attribute (random forests and XGboost) as well SVC with a linear kernel. Whatever you use, this is how you access them.\nFor this example, I will use the dataset but only using 14 wavenumbers from González Jiménez et al. (2019).\n\n\n# Import data\ndf = pd.read_csv(\"../data/UV_pilot.csv\")\n\nwavenumbers = [\n    \"3856\",\n    \"3401\",\n    \"3275\",\n    \"2923\",\n    \"2859\",\n    \"1902\",\n    \"1745\",\n    \"1636\",\n    \"1538\",\n    \"1457\",\n    \"1307\",\n    \"1153\",\n    \"1076\",\n    \"1027\",\n    \"881\",\n    \"527\",\n    \"401\",\n]\n\n\n# Split features and target\nX = df[wavenumbers]\ny = df.loc[:, \"Exposed\"]\n\n# Split into train and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, shuffle=True, stratify=y, test_size=0.2\n)\n\nSource: basic_ml_analysis.ipynb\nI have already selected an optimized pipeline.\n\n\noptimized_model = search.best_estimator_\noptimized_model.fit(X_train, y_train)\n\nPipeline(steps=[('model',\n                 LogisticRegression(C=1, class_weight='balanced',\n                                    max_iter=10000, solver='liblinear'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('model',\n                 LogisticRegression(C=1, class_weight='balanced',\n                                    max_iter=10000, solver='liblinear'))])  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(C=1, class_weight='balanced', max_iter=10000,\n                   solver='liblinear') \n\n\nSource: basic_ml_analysis.ipynb\nYou can access the attributes of each part of the pipeline using named_steps function.\n\n\n# accessing the model coefficients\noptimized_model.named_steps['model'].coef_\n\narray([[-0.01086563, -0.06647627, -0.04028753,  0.03905258,  0.04798557,\n        -0.0115063 ,  0.02188028,  0.02296419,  0.04247399,  0.03510987,\n         0.03820413, -0.08391647,  0.07831322,  0.01641863, -0.04101555,\n        -0.07048381, -0.13402034]])\n\n\nSource: basic_ml_analysis.ipynb\nThe number of coefficients will be equal to the number of features. They will be ordered the same as you input as targets.\nYou can create a dataframe with the\n\n\nfeature_importance = pd.DataFrame()\nfeature_importance['Wavenumbers'] = wavenumbers\nfeature_importance['Coefficients'] = optimized_model.named_steps['model'].coef_.T\nfeature_importance['Wavenumbers'] = feature_importance['Wavenumbers'].astype('category')\nfeature_importance\n\n\n\n\n\n\n\n\nWavenumbers\nCoefficients\n\n\n\n\n0\n3856\n-0.010866\n\n\n1\n3401\n-0.066476\n\n\n2\n3275\n-0.040288\n\n\n3\n2923\n0.039053\n\n\n4\n2859\n0.047986\n\n\n5\n1902\n-0.011506\n\n\n6\n1745\n0.021880\n\n\n7\n1636\n0.022964\n\n\n8\n1538\n0.042474\n\n\n9\n1457\n0.035110\n\n\n10\n1307\n0.038204\n\n\n11\n1153\n-0.083916\n\n\n12\n1076\n0.078313\n\n\n13\n1027\n0.016419\n\n\n14\n881\n-0.041016\n\n\n15\n527\n-0.070484\n\n\n16\n401\n-0.134020\n\n\n\n\n\n\n\nSource: basic_ml_analysis.ipynb\nFinally, you plot as a barplot. The values of the coefficients can be positive or negative. If they are positive they push the decisiion to the negative class.\n\n\nfeature_importance.plot.barh(x='Wavenumbers', y='Coefficients')\n\n&lt;Axes: ylabel='Wavenumbers'&gt;\n\n\n\n\n\nSource: basic_ml_analysis.ipynb\nAnd there you have it. If your model has a feature importance attribute, the same principle applies. There are other approaches such as permutation importance which can be used when you are using discrete numbers of features.\n\n\n3.1.5 Permutation importance\nSklearn has a permutation importance class that you can use. Basically, a baseline accuracy is calculated with an unseen test set. Then, a feature is selected and permutated, the new accuracy is calculate and the permutation importance is given by the diffferentece between the baseline and the permutated accuracy. You repeat the process n times and you can then check the values and see what feature caused the most decrease in accuracy.\n\n\n# permutation importance\nfrom sklearn.inspection import permutation_importance\nr = permutation_importance(optimized_model, X_test, y_test,\n                           n_repeats=200,\n                           random_state=0, scoring='accuracy')\n\nSource: basic_ml_analysis.ipynb\nYou can plot the mean with the standard deviation pretty easily\n\n\n# create a series of the importances mean and add the names of the wavenumber values\nforest_importances = pd.Series(r.importances_mean, index=X.columns.values)\n\n# Barplot with sd bars\n#forest_importances.plot.bar(yerr=r.importances_std)\n\nSource: basic_ml_analysis.ipynb\nHere, we can see how the absorbance at 1076 cm\\(^{-1}\\) caused a major decrease in accuracy compared to the rest of wavenumber values.\nIf we compared the two approaches:\n\n\n# comparing the two approaches\n\nfig, ax = plt.subplots(tight_layout=True)\n\nfeature_importance.plot.bar(x='Wavenumbers', y='Coefficients', ax=ax, color='r')\n#forest_importances.plot.bar(yerr=r.importances_std, ax=ax2, color='g')\n\n&lt;Axes: xlabel='Wavenumbers'&gt;\n\n\n\n\n\nSource: basic_ml_analysis.ipynb\nSome wavenumbers matches, some other do not.\n\n\n3.1.6 Permutation importance using RF\nLet’s do the same process but with a RF model. After we created the pipeline and and test it on the test set. We got an accuracy of 0.7. We access the feature importance using the attribute feature_importances_\n\n\npipe.named_steps['model'].feature_importances_\n\narray([0.17093772, 0.04011147, 0.02748989, 0.06387667, 0.07254563,\n       0.08291522, 0.05297084, 0.06783495, 0.05192696, 0.05611174,\n       0.03902881, 0.05445541, 0.06264931, 0.04480644, 0.03412516,\n       0.04047971, 0.03773409])\n\n\nSource: basic_ml_analysis.ipynb\nWe can plot the values vs wavenumber values\n\n\nfeature_importance = pd.DataFrame()\nfeature_importance['Wavenumbers'] = wavenumbers\nfeature_importance['FI'] = pipe.named_steps['model'].feature_importances_\nfeature_importance['Wavenumbers'] = feature_importance['Wavenumbers'].astype('category')\nfeature_importance.plot.bar(x='Wavenumbers', y='FI')\n\n&lt;Axes: xlabel='Wavenumbers'&gt;\n\n\n\n\n\nSource: basic_ml_analysis.ipynb\nThe absorbance at 3856 cm\\(^{-1}\\) seem to be quite important for this model. If we compare to the permutation importance we can see that they are not similar.\n\n\n\nText(0, 0.5, 'Feature importance permutation')\n\n\n\n\n\nSource: basic_ml_analysis.ipynb\nYou can also compare between using the train set or the test set for permutatioin importance:\n\n\nfig, (ax, ax2) = plt.subplots(2,1,figsize=(10,5), tight_layout=True)\nimportances.plot.box(vert=True, whis=10, ax=ax, color='r')\nax.axhline(0, ls='--', color='k')\n\nimportances_train.plot.box(vert=True, whis=10, ax=ax2, color='b')\nax.axhline(0, ls='--', color='k')\n\n\nax.set_xlabel(\"Wavenumber\")\nax.set_ylabel(\"Decrease in accuracy\")\nax.set_title(\"Permutation Importances (test set)\")\n\nax2.set_xlabel(\"Wavenumber\")\nax2.set_ylabel(\"Decrease in accuracy\")\nax2.set_title(\"Permutation Importances (train set)\")\n\nText(0.5, 1.0, 'Permutation Importances (train set)')\n\n\n\n\n\nSource: basic_ml_analysis.ipynb\nSomething to consider and I quote:\n“Permutation importances can be computed either on the training set or on a held-out testing or validation set. Using a held-out set makes it possible to highlight which features contribute the most to the generalization power of the inspected model. Features that are important on the training set but not on the held-out set might cause the model to overfit.”\n\n\n\n\n\n\nWarning\n\n\n\nFeatures that are deemed of low importance for a bad model (low cross-validation score) could be very important for a good model. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model (sklearn documentation)"
  },
  {
    "objectID": "mlpipelines.html#nested-cross-validation",
    "href": "mlpipelines.html#nested-cross-validation",
    "title": "3  Machine learning pipelines",
    "section": "3.2 Nested Cross validation",
    "text": "3.2 Nested Cross validation\nNested cross validation can be useful when you do not have a large sample size. Improper use of validation approaches can result in biases in our results. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. Graphical examples of how different validations techniques are shown in Figure 3.1. Nested cross-validation has a inner and outer layer. The inner layer will be used to optimize the model and the outer layer will be used to test the optimized model. This process will be repeated n times. Bear in mind that this is a very time and source consuming approach. You can read more on which different approaches you can use to deal with small data sets [Vabalas et al. (2019)](Varma and Simon 2006). For this example we are going to use nested cross validation as shown in Figure 3.1. We take all the data, and we split it in n folds (outer layer), n-1 folds will be used for model development/optimization (inner layer) and the reminder fold will be used to validate the model. This process will be repeated for each fold in the outer layer. The final accuracy will be the mean of each outer layer.\n\n\n\nFigure 3.1: validation-techniques\n\n\nValidation methods. A: Train/Test Split. B: K-Fold CV. C: Nested CV. D: Partially nested CV. ACC—overall accuracy of the model, ACCi.—accuracy in a single CV fold (Taken from (Vabalas et al. 2019))\nAs we assumed that we do not have enough samples in our data set, therefore, we do not need to split the data into train and test sets.\n\n\n# Split features and target\nX = df.loc[:,\"4000\":\"403\"]\ny = df.loc[:,\"Exposed\"]\n\nSource: nested_ml.ipynb\nWe set up the conditions of our nested cross validations. We add our model and any preprocessing, the number of splits for the inner and outer layer and our final pipeline. This time, we are using KFold and 3 folds for both layers\n\n\nscaler = StandardScaler(with_std=False)\nmodel = LogisticRegression(max_iter=10000)\nsplits_outer = 3\nsplits_inner = 3\n\npipe = Pipeline(steps=[(\"scaler\", scaler), (\"model\", model)])\n\n# configure nested cross-validation layers\ncv_outer = KFold(n_splits=splits_outer, shuffle=True, random_state=123)\ncv_inner = KFold(n_splits=splits_inner, shuffle=True, random_state=123)\n\nSource: nested_ml.ipynb\nNow, the for loop. First, we split the data using the outer split,with for train_ix, test_ix in cv_outer.split(X):. We run GridSearchCV using the train test with the inner layer cross validation (specified in the parameter cv of GridSearchCV). The best model is then tested on the test set of the outer layer.\n\n\n# create confusion matrix list to save each of external cv layer\ncm_nested = []\n\n# enumerate splits and create AUC plot\nyhat_nested = []\ny_test_nested = []\nX_test_nested = []\nbest_estimators = []\nmean_fpr = np.linspace(0, 1, 100)\nouter_results = list()\n\n\nfor train_ix, test_ix in cv_outer.split(X):\n# split data\n    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n    y_train, y_test = y[train_ix], y[test_ix]\n\n    # define search space\n    param_grid = {'model__penalty': ['l1', 'l2'],\n                  'model__C': [0.01, 0.1, 1, 10],\n                  'model__solver': ['liblinear', 'saga']\n                  }\n    # define search\n\n    search = GridSearchCV(pipe, param_grid, scoring='accuracy', cv=cv_inner, refit=True)\n    # execute search\n    result = search.fit(X_train, y_train)\n    \n    # get the best performing model fit on the whole training set\n    best_model = result.best_estimator_\n\n    # create pipeline with the best model\n    best_pipe = Pipeline(steps=[(\"scaler\", scaler), (\"best_model\", best_model)])\n    best_pipe.fit(X_train, y_train)\n    # evaluate model on the hold out dataset\n\n    yhat = best_pipe.predict(X_test)\n\n    # evaluate the model\n    acc = accuracy_score(y_test, yhat)\n    cm = confusion_matrix(y_test, yhat)\n    yhat_nested.append(yhat)\n\n    y_test_nested.append(y_test)\n    X_test_nested.append(X_test)\n    \n     # store the result\n    outer_results.append(acc)\n    cm_nested.append(cm)\n    best_estimators.append(best_pipe)\n    \n    print(f'Accuracy = {result.best_score_:.2f}, Parameters={result.best_params_}')\n    \n# summarize the estimated performance of the model\nprint(f'Final accuracy = {np.mean(outer_results):.2f} ±  {np.std(outer_results):.2f}')\n\nAccuracy = 0.68, Parameters={'model__C': 10, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\nAccuracy = 0.54, Parameters={'model__C': 0.01, 'model__penalty': 'l2', 'model__solver': 'saga'}\nAccuracy = 0.63, Parameters={'model__C': 10, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\nFinal accuracy = 0.62 ±  0.12\n\n\nSource: nested_ml.ipynb\nTo report the result you can use AUC-ROC with confidence intervals or a confusion matrix with the mean of the accuracies of each of the results from the outer layer. Let’s plot the confusion matrix\n\n\n# sum all the confusion matrices\ncm_sum = np.sum(cm_nested,axis=0)\n\n# we normalised the confusion matrix\ncm_sum_normalized = cm_sum.astype('float')/cm_sum.sum(axis=1)[:, np.newaxis]\n                                                              \n# Plot the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_sum_normalized, display_labels=best_model.classes_)\ndisp.plot(cmap = \"PuBu\")\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x11c952840&gt;\n\n\n\n\n\nSource: nested_ml.ipynb\nTo plot the AUC-ROC with confidence intervals is a little bit tricky using nested cross validation. We need to create lists where we store the best estimator and the X_train, and y_train sets of each outer layer. Then we run a for loop where we going to plot the roc curve for each X_train, y_test and best model using RocCurveDisplay.from_estimator function.\n\n\n# AUC-ROC curve\n\nmean_fpr = np.linspace(0, 1, 100)\ntprs = []\naucs = []\n\n\nfig, ax = plt.subplots()\nfor a, b, c in zip(X_test_nested,y_test_nested, best_estimators):\n        viz = RocCurveDisplay.from_estimator(\n        c,\n        a,\n        b,\n        alpha=0.4,\n        lw=1,\n        plot_chance_level = True,\n        ax=ax)\n\n        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        aucs.append(viz.roc_auc)\n\n\n\n\nSource: nested_ml.ipynb\nWe can see three curves that correspond to each outer layer and we can see the values of AUC for each one in the plot legend. Now we need to calculate the mean of the curves plot it over the current plot\n\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\n\nfig, ax = plt.subplots()\nfor a, b, c in zip(X_test_nested,y_test_nested, best_estimators):\n        viz = RocCurveDisplay.from_estimator(\n        c,\n        a,\n        b,\n        alpha=0.4,\n        lw=1,\n        plot_chance_level = True,\n        ax=ax)\n\n        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        aucs.append(viz.roc_auc)\n\nax.plot(\n        mean_fpr,\n        mean_tpr,\n        color=\"darkblue\",\n        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n        lw=2.5,\n        alpha=1)\n\n\n\n\n\nSource: nested_ml.ipynb\nFinally, we plot the standard deviation and get rid of the legend\n\n\nfig, ax = plt.subplots()\nfor a, b, c in zip(X_test_nested,y_test_nested, best_estimators):\n        viz = RocCurveDisplay.from_estimator(\n        c,\n        a,\n        b,\n        alpha=0.4,\n        lw=1,\n        plot_chance_level = True,\n        ax=ax)\n\n        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        aucs.append(viz.roc_auc)\n\nax.plot(\n        mean_fpr,\n        mean_tpr,\n        color=\"darkblue\",\n        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n        lw=2.5,\n        alpha=1)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nax.fill_between(\n        mean_fpr,\n        tprs_lower,\n        tprs_upper,\n        color=\"grey\",\n        alpha=0.2)\n\nax.get_legend().remove()\n\n\n\n\nSource: nested_ml.ipynb"
  },
  {
    "objectID": "mlpipelines.html#nested-cross-validation-plus-validation-on-unseen-data",
    "href": "mlpipelines.html#nested-cross-validation-plus-validation-on-unseen-data",
    "title": "3  Machine learning pipelines",
    "section": "3.3 Nested cross validation plus validation on unseen data",
    "text": "3.3 Nested cross validation plus validation on unseen data\nThis is a variation of the nested cross validation (develop by our colleagues from IHI). The addition is that you will test the final model in unseen data. So, you can report both, your results of the nested cross-validation as well as the accuracy on the unseen data.\nHere we are going to use:\n\nUsing numpy arrays instead of dataframes\nSplit the data into train and test as a whole\nSaving a lot of data from the cross validation to further explore the results.\nExport and save the model (to avoid training every time we run the notebook)\nSetting up the general aesthetics of the plots at the beginning of the script\n\nFor easiness, we have contain the whole code into a function, to avoid mistakes when copy and paste from different notebooks\n\n3.3.1 Set plot aesthetics\nSetting up the general aesthetics of the plot. When using the seaborn package, you can set the style, context fonts, of all the plots from the beginning\nsns.set(\n    context = \"paper\",\n    style = \"white\",\n    palette = \"deep\",\n    font_scale = 2.0,\n    color_codes = True,\n    rc = ({\"font.family\": \"Dejavu Sans\"})\n    )\n\n\n3.3.2 Defining the functions\nWe defined two functions: ml_loop and confusion_matrix_mau.\n\n\n# define functions\n\ndef ml_loop(\n    X_train,\n    y_train_species,\n    kf,\n    num_rounds: int,\n    random_grid: dict,\n    scoring: str,\n    species_model,\n    name \n):\n    \"\"\"\n    Perform a machine learning loop with cross-validation and randomized grid search.\n\n    This function performs a machine learning loop that includes cross-validation,\n    randomized grid search for hyperparameter tuning, and model evaluation. It\n    trains and evaluates the model multiple times (specified by num_rounds) and\n    collects the results.\n\n    Parameters\n    ----------\n    X_train : array-like of shape (n_samples, n_features)\n        The training input samples.\n    y_train_species : array-like of shape (n_samples,)\n        The true labels for the training samples.\n    kf : cross-validation generator\n        A cross-validation generator, such as KFold or StratifiedKFold.\n    num_rounds : int\n        The number of rounds to run the loop.\n    random_grid : dict\n        The hyperparameter grid for randomized search.\n    scoring : str\n        The scoring metric to evaluate the model, e.g., 'accuracy'.\n    species_model : estimator object\n        The machine learning model to be used, e.g., an instance of SVC.\n\n    Returns\n    -------\n    species_predicted : list\n        A list of predicted labels for the test samples across all rounds.\n    species_true : list\n        A list of true labels for the test samples across all rounds.\n    kf_results : pd.DataFrame\n        A DataFrame containing the model parameters and global accuracy scores.\n    kf_per_class_results : list\n        A list of per-class accuracy scores for\n    \n\n    \"\"\"\n    start = time()\n    kf_results = pd.DataFrame() # model parameters and global accuracy score\n    kf_per_class_results = []\n    species_predicted, species_true = [], []\n    \n    for vuelta in range(num_rounds):\n        print(f\"\\nThis is round {vuelta+1}...\\n\")\n        SEED = np.random.randint(0, 81478)\n\n        # Before your cross-validation loop\n        # y_species_train = np.where(y_species_train == 'AA', 0, 1)\n\n        # Inside your cross-validation loop, this step is not needed as the data is already encoded\n\n        # cross validation and splitting of the validation set\n        for train_index, test_index in kf.split(X_train, y_train_species):\n            X_train_set, X_test_set = X_train[train_index], X_train[test_index]\n            y_train_species_set, y_val_species_set = (\n                y_train_species[train_index],\n                y_train_species[test_index],\n            )\n\n            # print('The shape of X train set : {}'.format(X_train_set.shape))\n            # print('The shape of y train species : {}'.format(y_train_species_set.shape))\n            # print('The shape of X test set : {}'.format(X_test_set.shape))\n            # print('The shape of y test species : {}\\n'.format(y_val_species_set.shape))\n\n            # generate models using all combinations of settings\n\n            # RANDOMSED GRID SEARCH\n            # Random search of parameters, using 5 fold cross validation,\n            # search across 100 different combinations, and use all available cores\n\n            n_iter_search = 10\n            rsCV = RandomizedSearchCV(\n                verbose=1,\n                estimator=species_model,\n                param_distributions=random_grid,\n                n_iter=n_iter_search,\n                scoring=scoring,\n                cv=kf,\n                refit=True,\n                n_jobs=-1,\n            )\n\n            rsCV_result = rsCV.fit(X_train_set, y_train_species_set)\n\n            # print out results and give hyperparameter settings for best one\n            means = rsCV_result.cv_results_[\"mean_test_score\"]\n            stds = rsCV_result.cv_results_[\"std_test_score\"]\n            params = rsCV_result.cv_results_[\"params\"]\n            # for mean, stdev, param in zip(means, stds, params):\n            # print(\"Accuracy of %.2f $\\pm$(%.2f) with: %r\" % (mean, stdev, param))\n\n            # print best parameter settings\n            print(\n                \"Best accuracy: %.2f using %s\"\n                % (rsCV_result.best_score_, rsCV_result.best_params_)\n            )\n\n            # Insert the best parameters identified by randomized grid search into the base classifier\n            species_classifier = species_model.set_params(**rsCV_result.best_params_)\n\n            # Fit your models\n            species_classifier.fit(X_train_set, y_train_species_set)\n\n            # predict test instances\n            sp_predictions = species_classifier.predict(X_test_set)\n\n            # zip all predictions for plotting averaged confusion matrix\n            # species\n            for predicted_sp, true_sp in zip(sp_predictions, y_val_species_set):\n                species_predicted.append(predicted_sp)\n                species_true.append(true_sp)\n\n            # species local confusion matrix & classification report\n            local_cm_species = confusion_matrix(y_val_species_set, sp_predictions)\n            local_report_species = classification_report(\n                y_val_species_set, sp_predictions\n            )\n\n            # append feauture importances\n            # local_feat_impces_species = pd.DataFrame(species_classifier.feature_importances_,\n            #                                    index = features.columns).sort_values(by = 0, ascending = False)\n\n            # summarizing results\n            local_kf_results = pd.DataFrame(\n                [\n                    (\n                        \"Accuracy_species\",\n                        accuracy_score(y_val_species_set, sp_predictions),\n                    ),\n                    (\"TRAIN\", str(train_index)),\n                    (\"TEST\", str(test_index)),\n                    (\"CM\", local_cm_species),\n                    (\"Classification report\", local_report_species),\n                    (\"y_test\", y_val_species_set),\n                    # (\"Feature importances\", #local_feat_impces_species.to_dict())\n                ]\n            ).T\n\n            local_kf_results.columns = local_kf_results.iloc[0]\n            local_kf_results = local_kf_results[1:]\n            kf_results = pd.concat(\n                [kf_results, local_kf_results], axis=0, join=\"outer\"\n            ).reset_index(drop=True)\n\n            # per class accuracy\n            local_support = precision_recall_fscore_support(\n                y_val_species_set, sp_predictions\n            )[3]\n            local_acc = np.diag(local_cm_species) / local_support\n            kf_per_class_results.append(local_acc)\n    elapsed = time() - start\n    print(f\"\\nTime elapsed: {elapsed:.2f} seconds\")\n\n    filename = \"./results/models/trained_model_\" + name + \".sav\"\n    joblib.dump(species_classifier, filename)\n\n    return species_true, species_predicted, kf_results\n\ndef confusion_matrix_mau(y_pred, y_true, xticks_rotation=None, ax=None):\n    \"\"\"\n    Displays a normalized confusion matrix using the true and predicted labels.\n\n    This function generates and displays a normalized confusion matrix using the\n    true labels (`y_true`) and the predicted labels (`y_pred`). The confusion\n    matrix is displayed using a grayscale colormap and is normalized to show\n    proportions instead of raw counts.\n\n    Args:\n        y_pred (array-like of shape (n_samples,)): The predicted labels.\n        y_true (array-like of shape (n_samples,)): The true labels.\n        ax (matplotlib.axes.Axes, optional): The axes on which to plot the confusion matrix.\n            If None, the current axes will be used.\n\n    Returns:\n        None\n    \"\"\"\n    ConfusionMatrixDisplay.from_predictions(y_pred=y_pred, y_true=y_true,normalize='true', values_format='.2f', cmap=plt.cm.Greys, ax=ax, im_kw={'vmin':0, 'vmax':1}, xticks_rotation=xticks_rotation)\n\nSource: expanded_nested.ipynb\nThe only new package here is joblib to save our trained model. The principle is the same as nested cross validation.\n\n\n3.3.3 Split the data\nWe load our data and split into train and test set. We also stratify the split using the “Exposed” variable and then separate our features and target from each of the splits\nX_train = np.asarray(train_set.iloc[:,5:-1]) # feature matrix\ny_train_exposure = np.asarray(train_set['Exposed'])\n\nX_test = np.asarray(test_set.iloc[:,5:-1])\ny_test_exposure = np.asarray(test_set['Exposed'])\nWhy are we using numpy arrays instead of dataframes? Numpy arrays provide a consistent and standardized data format that scikit-learn classifiers can easily handle. This allows for integration with other scikit-learn functionalities, such as preprocessing, cross-validation, and model evaluation\n\n\n\n\n\n\nTip\n\n\n\nSklearn classifiers can easily handle numpy arrays\n\n\n\n\n3.3.4 Setting the parameters of nested cross validation\nHere, we defined the number of folds (inner layer) and the number of rounds (outer layer). Note that we are using a random number generator as random_seed. This number will change each round and will give use a different split in the outer layer\n# Set validation procedure\nnum_folds = 5 # split training set into 5 parts for validation\nnum_rounds = 2 # increase this to 5 or 10 once code is bug-free\nseed = 42 # pick any integer. This ensures reproducibility of the tests\nrandom_seed = np.random.randint(0, 81478)\n\nscoring = 'accuracy' # score model accuracy\n\n# cross validation strategy\nkf = KFold(\n        n_splits = num_folds, \n        shuffle = True, \n        random_state = random_seed\n        )\n\n\n3.3.5 Setting the parameters of nested cross validation\nThe hyperparameters for support vector machine that we are going to test are C, kernel and gamma.\nrandom_grid = {'C': [10, 1.0, 0.1, 0.01], 'kernel': [\"linear\", 'rbf', 'poly'], 'gamma': [0.1, 1, 10]}\n\nspecies_model = SVC()\n\n\n3.3.6 Run the loop\nWe run the loop. The function needs X and y train, the split type, number of rounds and folds, hyperparameters, the score and the model\n\n\n# Species prediction\n\ny_mau_true, y_mau_predicted, main_results = ml_loop(X_train, y_train_exposure, kf, num_rounds=num_rounds, random_grid=random_grid, scoring='accuracy', species_model=exposure_model, name=\"mau_mau\" )\n\n\nThis is round 1...\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.78 using {'kernel': 'poly', 'gamma': 10, 'C': 10}\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.75 using {'kernel': 'poly', 'gamma': 10, 'C': 1.0}\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.86 using {'kernel': 'poly', 'gamma': 1, 'C': 10}\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.77 using {'kernel': 'poly', 'gamma': 10, 'C': 0.1}\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.74 using {'kernel': 'poly', 'gamma': 1, 'C': 1.0}\n\nThis is round 2...\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.78 using {'kernel': 'poly', 'gamma': 10, 'C': 1.0}\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.75 using {'kernel': 'poly', 'gamma': 1, 'C': 10}\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.86 using {'kernel': 'poly', 'gamma': 10, 'C': 0.01}\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.77 using {'kernel': 'poly', 'gamma': 10, 'C': 10}\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nBest accuracy: 0.74 using {'kernel': 'poly', 'gamma': 1, 'C': 1.0}\n\nTime elapsed: 3.40 seconds\n\n\nSource: expanded_nested.ipynb\nYou can see how each round will contain 5 best models.\nWe plot a confusion matrix which is the sum of all the confusion matrices from each round.\n\n\n# this function make pretty confusion matrices\nconfusion_matrix_mau(y_true=y_mau_true, y_pred=y_mau_predicted)\n\n\n\n\nSource: expanded_nested.ipynb\nWe load our trained model, test it in the unseen data and plot the confusion matrix\nloaded_model_exposure = joblib.load('./results/models/trained_model_mau_mau.sav')\n\n\ny_hd_pred = loaded_model_exposure.predict(X_test)\nacc = accuracy_score(y_true=y_test_species, y_pred=y_hd_pred)\nprint(f\"Accuracy on test set using the head: {acc}\")\n\nconfusion_matrix_mau(y_true=y_test_species, y_pred=y_hd_pred)\n\nAccuracy on test set using the head: 0.65\n\n\n\n\n\nSource: expanded_nested.ipynb\nSince we save a lot of data from the loop, for example accuracy of each inner layer. We can plot an histogram to check how stable was the accuracy across layers.\n\n\nsns.histplot(data=main_results, x='Accuracy_species',bins=12)\n\n&lt;Axes: xlabel='Accuracy_species', ylabel='Count'&gt;\n\n\n\n\n\nSource: expanded_nested.ipynb\n\n\n\n\nGonzález Jiménez, Mario, Simon A. Babayan, Pegah Khazaeli, Margaret Doyle, Finlay Walton, Elliott Reedy, Thomas Glew, et al. 2019. “Prediction of Mosquito Species and Population Age Structure Using Mid-Infrared Spectroscopy and Supervised Machine Learning [Version 3; Peer Review: 2 Approved].” Wellcome Open Research. https://doi.org/10.12688/wellcomeopenres.15201.3.\n\n\nVabalas, Andrius, Emma Gowen, Ellen Poliakoff, and Alexander J. Casson. 2019. “Machine Learning Algorithm Validation with a Limited Sample Size.” PLoS ONE 14 (11): e0224365. https://doi.org/10.1371/journal.pone.0224365.\n\n\nVarma, Sudhir, and Richard Simon. 2006. “Bias in Error Estimation When Using Cross-Validation for Model Selection.” BMC Bioinformatics 7 (1). https://doi.org/10.1186/1471-2105-7-91."
  },
  {
    "objectID": "deep_learning.html#a-basic-artificial-neural-network-for-species-classification",
    "href": "deep_learning.html#a-basic-artificial-neural-network-for-species-classification",
    "title": "4  Deep Learning pipelines",
    "section": "4.1 A basic artificial neural network for species classification",
    "text": "4.1 A basic artificial neural network for species classification\nThe most basic artifical neural network is the Multilayer Perceptron (MLP). It has following architecture (Figure 4.1):\n\nOne input layer\nOne or more hidden layers\nOne final layer called the output layer.\n\n\n\n\nFigure 4.1: mpl_architecture\n\n\nEvery layer except the output layer includes a bias neuron and is fully connected to the next layer.\nWe will start with a very simple ANN to predict two species of Anopheles mosquitoes using infrared spectroscopy.\n\n4.1.1 Load and preprocess the data\nAs mentioned early, we are going to use Tensorflow and Keras to do our deep learning analsysis. Let’s import the packages.\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import regularizers\nimport tensorflow_docs as tfdocs\nimport tensorflow_docs.modeling\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nOur dataset contains more than 10 categorical targets and around 1812 features.\n\n\n\n\n\n\n\n\n\n\nCat1\nCat2\nCat3\nCat4\nCat5\nCat6\nCat7\nCat8\nCat9\nCat10\n...\n420\n418\n416\n414\n412\n410\n408\n406\n404\n402\n\n\n\n\n0\nAC\nS\n0\nYY\nSU\nT1\nC1\nR1\n190823\n111223\n...\n0.2321\n0.2309\n0.2283\n0.2274\n0.2275\n0.2286\n0.2305\n0.2316\n0.2323\n0.2336\n\n\n1\nAC\nS\n0\nYY\nSU\nT1\nC1\nR1\n190823\n111223\n...\n0.2223\n0.2198\n0.2187\n0.2192\n0.2195\n0.2192\n0.2194\n0.2211\n0.2225\n0.2222\n\n\n2\nAC\nS\n0\nYY\nSU\nT1\nC1\nR1\n190823\n111223\n...\n0.2040\n0.2028\n0.2019\n0.2022\n0.2037\n0.2048\n0.2054\n0.2056\n0.2047\n0.2036\n\n\n3\nAC\nS\n0\nYY\nSU\nT1\nC1\nR1\n190823\n111223\n...\n0.2409\n0.2389\n0.2364\n0.2353\n0.2360\n0.2368\n0.2372\n0.2380\n0.2390\n0.2398\n\n\n4\nAC\nS\n0\nYY\nSU\nT1\nC1\nR1\n190823\n111223\n...\n0.2150\n0.2155\n0.2152\n0.2150\n0.2154\n0.2154\n0.2156\n0.2165\n0.2181\n0.2191\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4539\nAG\nS\n31\nYY\nSU\nT1\nK2\nR4\n190923\n30124\n...\n0.1638\n0.1634\n0.1618\n0.1608\n0.1606\n0.1607\n0.1617\n0.1625\n0.1623\n0.1616\n\n\n4540\nAG\nS\n31\nYY\nSU\nT1\nK2\nR4\n190923\n30124\n...\n0.2457\n0.2438\n0.2424\n0.2420\n0.2425\n0.2440\n0.2457\n0.2465\n0.2461\n0.2443\n\n\n4541\nAG\nS\n31\nYY\nSU\nT1\nK2\nR4\n190923\n30124\n...\n0.2676\n0.2667\n0.2678\n0.2682\n0.2674\n0.2683\n0.2712\n0.2717\n0.2695\n0.2685\n\n\n4542\nAG\nS\n31\nYY\nSU\nT1\nK2\nR4\n190923\n30124\n...\n0.2725\n0.2718\n0.2681\n0.2659\n0.2661\n0.2681\n0.2708\n0.2727\n0.2736\n0.2741\n\n\n4543\nAG\nS\n31\nYY\nSU\nT1\nK2\nR4\n190923\n30124\n...\n0.2230\n0.2238\n0.2225\n0.2199\n0.2185\n0.2193\n0.2213\n0.2231\n0.2230\n0.2223\n\n\n\n\n4544 rows × 1812 columns\n\n\n\nSource: deeplearning_primer.ipynb\nWe checked that our classes are balanced, which in this example they are not.\n\n\ndf.groupby(['Cat1'])['Cat2'].count().plot.bar()\n\n&lt;Axes: xlabel='Cat1'&gt;\n\n\n\n\n\nSource: deeplearning_primer.ipynb\nAfter we divide our data set into wavenumbers (X) and labels(y), we can use the imblearn package which allows to resample the class to match the underespresent class.\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=0)\n\nX_resampled, y_resampled = rus.fit_resample(X, y)\nprint(sorted(Counter(y_resampled).items()))\n\n[('AC', 1724), ('AG', 1724)]\n\n\nSource: deeplearning_primer.ipynb\nAlso, we need to encode our labels (transform species names into 1 or 0) using LabelBinarizer.\n# Change the labels into 0 or 1\nlb = LabelBinarizer()\ny_binary = lb.fit_transform(y_resampled)\nWe split our data set into train and test sets. We are going to use our test set for the final evaluation of our ANN model. It is good practice to scale the data, for this we are using standard scaling.\n# Split into train and test \nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_binary, stratify=y_binary, shuffle=True, test_size=0.2)\n\n# Scaling train and test\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n\n\n\n\nImportant\n\n\n\nWhen applying scaling, be sure you fit_transform the train test and only transform the test set. Moreover, scale the data after split it to avoid data leakege.\n\n\nWe further split the training set into training and validation.\n# Split it further train set into train and validation\nX_train_2, X_val, y_train_2, y_val = train_test_split(X_train, y_train, stratify=y_train, shuffle=True, test_size=0.2)\nOne of the things that you can do with keras is the use of callbacks. A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).\nYou can use callbacks to: Periodically save your model to disk Do early stopping, Get a view on internal states and statistics of a model during training, etc. Here, we are using it to avoid too much print messages during training.\n# functions to eliminate part of the messages when training ANN\ndef get_callbacks(name):\n  return [\n    tfdocs.modeling.EpochDots()\n  ]\n\n\n4.1.2 Building the model\nWe create our the model using the Sequential API as follows:\n# define the keras model\ninput_shape = [1796,]\nmodel = keras.Sequential()\nmodel.add(keras.layers.Input(shape=input_shape))\nmodel.add(keras.layers.Dense(500, activation='relu'))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\nLet’s go line by line: 1. The first line define the input of the model, which matches how many features (wavenumbers) we have in our data set. 2. Keras.sequential will create an object called ‘model’ in which we can add layers. 3. We add our first layer, the input layer. It has the shape of our previously define input 4. A first hidden layer, with 500 neurons and 'relu' activation 5. the final output layer with 1 neuron (becuase we are tackling a binary classification) with a sigmoid activation\n\n\n4.1.3 Compiling the model\nOnce the model is created, we compile it. Here, we defined the loss function (binary crossentropy), the optimizer (adam) and the metric (accuracy) to assess how well our model classifiy our data set.\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nwe can check the model using summary\n\n\nmodel.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                   │ (None, 500)            │       898,500 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 1)              │           501 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 899,001 (3.43 MB)\n\n\n\n Trainable params: 899,001 (3.43 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nSource: deeplearning_primer.ipynb\nIn the following table, you can see what parameters you can start on when building your models depeding on the classification problem.\n\nTypical classification MLP architecture\n\n\n\n\n\n\n\n\n\nHyperparameter\nRegression\nBinary classification\nMultilabel classification\nMulticlass classification\n\n\n\n\n# input neurons\nOne per input feature\nOne per input feature\nOne per input feature\nOne per input feature\n\n\n# hidden layers\n1 - 5\n1 - 5\n1 - 5\n1 - 5\n\n\n# neurons per hidden layer\n10 - 100\n10 - 100\n10 - 100\n10 - 100\n\n\n# output neurons\n1 per prediction dimension\n1\n1 per label\n1 per class\n\n\nHidden activation\nreLU\nreLU\nreLU\nreLU\n\n\nOutput activation\nNone\nLogistic\nLogistic\nSoftmax\n\n\nLoss function\nMSE\nCross entropy\nCross entropy\nCross entropy\n\n\n\n\n\n4.1.4 Training the model\nWe train the model with fit() function. Here, you specify you x and y, number of epochs, batch size and validation data. Also, you can add callbacks\n\n\nhistory = model.fit(x=X_train_2, y=y_train_2, epochs=500, batch_size=250, validation_data=[X_val, y_val],verbose=0,callbacks=get_callbacks('model_baseline'))\n\n\nEpoch: 0, accuracy:0.6011,  loss:1.5072,  val_accuracy:0.6159,  val_loss:0.9330,  \n....................................................................................................\nEpoch: 100, accuracy:0.9402,  loss:0.1599,  val_accuracy:0.8424,  val_loss:0.4303,  \n....................................................................................................\nEpoch: 200, accuracy:0.9465,  loss:0.1408,  val_accuracy:0.8351,  val_loss:0.5025,  \n....................................................................................................\nEpoch: 300, accuracy:0.9986,  loss:0.0207,  val_accuracy:0.8696,  val_loss:0.5299,  \n....................................................................................................\nEpoch: 400, accuracy:0.9995,  loss:0.0109,  val_accuracy:0.8750,  val_loss:0.5526,  \n....................................................................................................\n\n\nSource: deeplearning_primer.ipynb\nWe saved all the information of accuracy, loss when using the training set and validation set for each epoch in the history variable. You can access the information by using the history attribute. A good way to see if your model is overfitting or underfitting is by plotting the values of training/validation loss and accuracy.\n\n\n# Check training and validation curves\nfig, (ax, ax2) = plt.subplots(1, 2, figsize=(8,5), tight_layout=True)\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nax.plot(epochs, loss, 'r-', label='Training loss')\nax.plot(epochs, val_loss, 'b--', label='Validation loss')\nax.set_title('Training and validation loss')\nax.set_xlabel('Epochs')\nax.set_ylabel('Loss')\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nax2.plot(epochs, acc, 'r-', label='Training acc')\nax2.plot(epochs, val_acc, 'b--', label='Validation acc')\nax2.set_title('Training and validation accuracy')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('Accuracy')\n\nax.legend()\nax2.legend()\nax2.set_ylim(0,1.5)\n\n(0.0, 1.5)\n\n\n\n\n\nSource: deeplearning_primer.ipynb\n\n\n\n\n\n\nTip\n\n\n\nStart we few epochs to see how your computer handles the training. Currently, all these examples are tested on a Macbook pro M1 max pro with 32 GB of ram.\n\n\nWe are not going to discuss overfitting or undefitting in this section yet.\n\n\n4.1.5 Evaluate with unseen data\nWe got a pretty decent accuracy with a simple MPL model. Now, we evaluate on the test set\n\n\n# model evaluation\nmodel.evaluate(X_test, y_test)\n\n22/22 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8840 - loss: 0.4466\n\n\n[0.4332384169101715, 0.8927536010742188]\n\n\nSource: deeplearning_primer.ipynb\nWe got an accuracy of 0.89 which is pretty good.\nAs always, it is good to have confusion matrices to have a better understanding of the model perfomance. Therefore, we make predictions using X_test.\n\n\n# Make predictions\ny_proba = model.predict(X_test)\n\n22/22 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n\n\nSource: deeplearning_primer.ipynb\nWe round this predictions since they are probailities\n\n\n# Predictions will be a probability in the range between 0 and 1. So, we round them to have binary predictions\n\ny_predicted = y_proba.round(0).astype(int)\ny_predicted[0:5]\n\narray([[1],\n       [1],\n       [1],\n       [1],\n       [0]])\n\n\nSource: deeplearning_primer.ipynb\nand then we compute and plot the final confusion matrix\n\n\n# Confusion matrix\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_predictions(y_pred=lb.inverse_transform(y_predicted), y_true=lb.inverse_transform(y_test), normalize='true', cmap='Blues',im_kw={'vmin':0, 'vmax':1})\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x157215fa0&gt;\n\n\n\n\n\nSource: deeplearning_primer.ipynb\nBoth classes have quite high accuracy. So we are happy.\n\n\n4.1.6 Save your model\nTraining ANN can be quite long and computational expensive. Therefore, it is good practice to save it.\n\n\n# save your model\nmodel.save(\"./models/my_first_keras_model.keras\")\n\nSource: deeplearning_primer.ipynb\nYou can load your trained model again if you want to continue the project.\n\n\n# load your model\nmodel_new = keras.models.load_model(\"./models/my_first_keras_model.keras\")\n\nSource: deeplearning_primer.ipynb"
  },
  {
    "objectID": "deep_learning.html#a-basic-artificial-neural-network-for-multiclass-age-classification",
    "href": "deep_learning.html#a-basic-artificial-neural-network-for-multiclass-age-classification",
    "title": "4  Deep Learning pipelines",
    "section": "4.2 A basic artificial neural network for multiclass age classification",
    "text": "4.2 A basic artificial neural network for multiclass age classification\nThe main workflow is exactly the same as for a binary classification. We only need to do some preprocessing to the age classes so they can be read and used bt the neural network.\nFor this example, we subset the original data set using only An. gambiae data. We converted our ages that were numerical into three different categories.\n\n\ndf_gambiae['Age groups'].unique()\n\narray(['0 - 6d', '8 - 16d', '18 - 31d'], dtype=object)\n\n\nSource: deep_learning_primer_age.ipynb\nThe new age grups are not as imbalanced so we will pass. Otherwise, you need to balance the groups\n\n\n\n&lt;Axes: xlabel='Age groups'&gt;\n\n\n\n\n\nSource: deep_learning_primer_age.ipynb\n\n4.2.1 Encoding labels\nThis is where things get a little more complicated. Keras models do not support labels as it is (you cannot pass the y as categories). Therefore, to make it work we need to encode our labels in a specific way.\nFirst, encode our target labels with values between 0 and n_classes-1. For this, we use LabelEncoder\n\n\n# Encode the labels from 0 to n-1 classes\nencoder = LabelEncoder()\nencoded_y = encoder.fit_transform(y)\nencoder.classes_\n\narray(['0 - 6d', '18 - 31d', '8 - 16d'], dtype=object)\n\n\nSource: deep_learning_primer_age.ipynb\nEach age group is an integer, '0 - 6d' is 0, '18 - 31d' is 1 and '8 - 16d' is 2.\nWe need to transform the encoded labels into a binary matrix. This is called OneHotEncoding\n\n\n# transform the encoded labels into a binary class matrix\nyhot = utils.to_categorical(encoded_y)\nprint(yhot)\nprint(yhot.shape)\n\n[[1. 0. 0.]\n [1. 0. 0.]\n [1. 0. 0.]\n ...\n [0. 1. 0.]\n [0. 1. 0.]\n [0. 1. 0.]]\n(2820, 3)\n\n\nSource: deep_learning_primer_age.ipynb\nAs you can see, each age group has 3 values. For the first sample, the values [1. 0. 0.] means that the sample belongs to the group 0 or 0 - 6d. The last sample [0. 1. 0.] belongs to the group 1 or 18 - 31d.\n\n\n4.2.2 Building the model\nFor this model we need to change the neurons of the last layer to match the number of labels we have, in this case is 3 and change the activation function to softmax. Moreover, we need to use categorical_crossentropy as loss function. And that’s all\n\n\ninput_shape = [1800,]\nmodel = keras.Sequential()\nmodel.add(keras.layers.Input(shape=input_shape))\nmodel.add(keras.layers.Dense(500, activation='relu'))\nmodel.add(keras.layers.Dense(3, activation='softmax'))\n\n# compile the keras model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nSource: deep_learning_primer_age.ipynb\nAfter training the model we can see that we got pretty high accuracies.\n\n\n\n(0.0, 1.5)\n\n\n\n\n\nSource: deep_learning_primer_age.ipynb\n\n\n4.2.3 Evaluate the model with useen data\nWe evaluate the model using unseen data and we got and accuracy of 82%.\n\n\n\n18/18 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8238 - loss: 0.6882\n18/18 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \n\n\nSource: deep_learning_primer_age.ipynb\n\n\n4.2.4 Reverse the encoding\nWe need to compute the confusion matrix and for that we need to reverse the encoding of our predicted vector and our test vector.\nANN will give us the probability of the sample to belong to each of the 3 categories.\n\n\n# the model gave us the proability of that samples to be 0, 1 or 2 category\nprint(y_proba[0])\n\n[1.9061598e-07 6.7195423e-02 9.3280441e-01]\n\n\nSource: deep_learning_primer_age.ipynb\nHere, we can see that this sample belongs to the category 2 or 8 - 16 d\nTo change to a vector of 1 dimension with the values of each category before we applied one hot encoding, we use the functon np.argmax which will go through each row and give us the index of the highest value.\n\n\n# Select the index with the highest value\ny_pred = np.argmax(y_proba, axis=1) \ny_pred\n\narray([2, 2, 1, 2, 0, 0, 0, 2, 0, 2, 2, 0, 2, 1, 1, 0, 0, 1, 1, 2, 2, 0,\n       2, 2, 0, 1, 0, 0, 2, 0, 1, 1, 0, 2, 2, 0, 0, 0, 2, 0, 1, 2, 2, 2,\n       2, 1, 1, 0, 2, 2, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 2, 2, 2, 0,\n       2, 2, 0, 0, 2, 0, 2, 0, 0, 2, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1, 0, 2,\n       2, 1, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 0, 1, 1, 0, 1, 2, 1, 0, 0, 2,\n       1, 1, 1, 0, 1, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 2, 1,\n       0, 1, 2, 1, 2, 0, 0, 0, 2, 2, 1, 0, 1, 2, 2, 0, 2, 2, 0, 1, 1, 1,\n       2, 0, 2, 1, 0, 0, 0, 2, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2,\n       2, 1, 1, 0, 1, 1, 0, 2, 0, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 2,\n       2, 0, 0, 0, 2, 2, 1, 1, 0, 0, 1, 1, 2, 0, 1, 2, 0, 0, 2, 2, 2, 0,\n       0, 1, 1, 1, 0, 1, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 1, 2, 0, 2, 0,\n       0, 2, 2, 0, 0, 0, 0, 2, 2, 1, 0, 2, 1, 2, 0, 2, 1, 0, 1, 0, 1, 1,\n       2, 2, 0, 0, 0, 2, 2, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 2, 1, 1, 1, 0,\n       2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 1, 2, 0, 2, 2, 1, 0, 0,\n       1, 1, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n       1, 2, 1, 2, 1, 2, 0, 0, 2, 2, 2, 2, 0, 2, 1, 1, 1, 1, 2, 2, 0, 0,\n       2, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 2, 0, 0, 1, 1, 0, 2, 2,\n       2, 0, 2, 2, 0, 1, 1, 2, 1, 0, 2, 2, 0, 2, 0, 1, 1, 1, 1, 0, 2, 1,\n       0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0,\n       1, 2, 0, 0, 0, 2, 2, 1, 1, 2, 2, 1, 0, 2, 2, 0, 1, 0, 1, 0, 1, 2,\n       2, 1, 0, 0, 1, 2, 0, 2, 0, 2, 1, 1, 2, 0, 0, 1, 1, 0, 2, 1, 1, 2,\n       1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 2, 1, 1, 2,\n       2, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 2, 2, 1, 0, 2, 2, 2,\n       1, 2, 0, 1, 0, 1, 1, 2, 0, 1, 0, 2, 1, 2, 1, 0, 0, 2, 1, 0, 2, 0,\n       1, 0, 0, 0, 2, 2, 1, 1, 2, 2, 1, 0, 2, 0, 0, 2, 0, 1, 2, 2, 2, 2,\n       0, 0, 1, 0, 2, 0, 2, 0, 1, 0, 0, 1, 2, 0])\n\n\nSource: deep_learning_primer_age.ipynb\nYou can see that you have a vector with the encoded categories from 0 to 2. We need to pass these indexes to the encoder. There are two ways of doing this, using encoder.classes_ attribute or the inverse_transform function.\n\n\n# reverse the encoding to obtaing the original classes\n# one way of doing is using the argument classes from the encoder and pass the indexes of the highest values of the probabilities for each class. \ny_pred_oneform = encoder.classes_[np.argmax(y_proba,axis=1)]\n\n\n# the second way is use the inverse transform function from the encoder to get the original classes\ny_pred_secform = encoder.inverse_transform(y_pred) \n\nSource: deep_learning_primer_age.ipynb\nBoth approaches give the same results\n\n\n# they are the same\nprint(y_pred_oneform[0])\nprint(y_pred_secform[0])\n\n8 - 16d\n8 - 16d\n\n\nSource: deep_learning_primer_age.ipynb\nWe apply the same process to the y_test and we compute our confusion matrix\n\n\n# Confusion matrix\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nConfusionMatrixDisplay.from_predictions(\n    y_pred=encoder.inverse_transform(y_pred),\n    y_true=encoder.inverse_transform(y_new_test),\n    normalize='true',\n    cmap='Blues',\n    im_kw={'vmin':0, 'vmax':1},\n    labels=['0 - 6d', '8 - 16d', '18 - 31d'])\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1047cdfd0&gt;\n\n\n\n\n\nSource: deep_learning_primer_age.ipynb\nWe pass a list of the age groops in the labels parameter to show the correct order of the labels. Otherwise, it the 8 - 16d group will appear last.\nif you want to learn more activations check this nice article\nThe simplest way to prevent overfitting is to start with a small model: A model with a small number of learnable parameters (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model’s “capacity”.\nIntuitively, a model with more parameters will have more “memorization capacity” and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power, but this would be useless when making predictions on previously unseen data.\nAlways keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.\nOn the other hand, if the network has limited memorization resources, it will not be able to learn the mapping as easily. To minimize its loss, it will have to learn compressed representations that have more predictive power. At the same time, if you make your model too small, it will have difficulty fitting to the training data. There is a balance between “too much capacity” and “not enough capacity”.\nUnfortunately, there is no magical formula to determine the right size or architecture of your model (in terms of the number of layers, or the right size for each layer). You will have to experiment using a series of different architectures.\nTo find an appropriate model size, it’s best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss.\nStart with a simple model using only densely-connected layers (tf.keras.layers.Dense) as a baseline, then create larger models, and compare them."
  },
  {
    "objectID": "deep_learning.html#evaluate-your-model-using-crossvalidation.",
    "href": "deep_learning.html#evaluate-your-model-using-crossvalidation.",
    "title": "4  Deep Learning pipelines",
    "section": "4.3 Evaluate your model using crossvalidation.",
    "text": "4.3 Evaluate your model using crossvalidation.\nSo far, all the evaluation has been done in a single split of the data sets.Your results will depend on how your data set is split everytime (unless you set a seed). We might want to see how stable is the model with different parts of the data set.\nThe approach is very similar to ML pipelines section. for this example, we are going to use the whole data set.\n\n4.3.1 Define the cross-validator\nFirst, we need to define our cross-validator. Here, we used Stratifiedkfold. The difference between this and Kfold is that Stratifeldkfold maitains the percetage of each class on each fold.\n\n\nsss = StratifiedKFold(n_splits=5,\n                      shuffle=True,\n                      random_state=123)\n\n# define the callbacks\ndef get_callbacks(name):\n  return [\n    tfdocs.modeling.EpochDots()\n  ]\n\nSource: deep_learning_cv.ipynb\n\n\n4.3.2 Cross validation process\nNow, the big loop. We create a dictionary called histories where we are going to save the results of each fold. We got the first split, and we print the fold number. Clear any model that we have made prviously to save memory (other wise you will get covered of all the models you create on each loop). We build our model, and compile it.\nWe create a new key in the dictionary histories that has the number of the fold and all the results of the fit. This process will happen 5 times. Each time, our model will be trained using for 2000 epochs. This process took 20 minutes in a high end computer so be careful before run the example. It is better if you start with few folds and the epochs beforehand.\n\n\nhistories = {}\nfor i, (train_index, test_index) in enumerate(sss.split(X_resampled, y_binary)):\n    print(f\"\\nFold {i}:\")\n    keras.backend.clear_session()\n    input_shape = [1796,]\n    model = keras.Sequential()\n    model.add(keras.layers.Input(shape=input_shape))\n    model.add(keras.layers.Dense(500, activation='relu'))\n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    histories[f\"fold{i}\"] = model.fit(x=X_resampled[train_index], y=y_binary[train_index], epochs=2000, batch_size=250, validation_data=[X_resampled[test_index], y_binary[test_index]],verbose=0,callbacks=get_callbacks('model_baseline'))\n\n\n\n\n\nFold 0:\n\nEpoch: 0, accuracy:0.5004,  loss:0.7859,  val_accuracy:0.5000,  val_loss:0.7151,  \n....................................................................................................\nEpoch: 100, accuracy:0.6302,  loss:0.6369,  val_accuracy:0.6261,  val_loss:0.6385,  \n....................................................................................................\nEpoch: 200, accuracy:0.6925,  loss:0.5858,  val_accuracy:0.6826,  val_loss:0.5977,  \n....................................................................................................\nEpoch: 300, accuracy:0.6958,  loss:0.5760,  val_accuracy:0.6290,  val_loss:0.6426,  \n....................................................................................................\nEpoch: 400, accuracy:0.7288,  loss:0.5394,  val_accuracy:0.7000,  val_loss:0.5660,  \n....................................................................................................\nEpoch: 500, accuracy:0.7194,  loss:0.5430,  val_accuracy:0.6870,  val_loss:0.5789,  \n....................................................................................................\nEpoch: 600, accuracy:0.7360,  loss:0.5223,  val_accuracy:0.6870,  val_loss:0.5534,  \n....................................................................................................\nEpoch: 700, accuracy:0.7509,  loss:0.5021,  val_accuracy:0.7101,  val_loss:0.5307,  \n....................................................................................................\nEpoch: 800, accuracy:0.7426,  loss:0.5058,  val_accuracy:0.7333,  val_loss:0.5299,  \n....................................................................................................\nEpoch: 900, accuracy:0.7571,  loss:0.4992,  val_accuracy:0.6841,  val_loss:0.5768,  \n....................................................................................................\nEpoch: 1000, accuracy:0.7596,  loss:0.4885,  val_accuracy:0.7145,  val_loss:0.5191,  \n....................................................................................................\nEpoch: 1100, accuracy:0.7774,  loss:0.4740,  val_accuracy:0.7435,  val_loss:0.5232,  \n....................................................................................................\nEpoch: 1200, accuracy:0.7574,  loss:0.4801,  val_accuracy:0.7319,  val_loss:0.5226,  \n....................................................................................................\nEpoch: 1300, accuracy:0.7455,  loss:0.5106,  val_accuracy:0.7464,  val_loss:0.4971,  \n....................................................................................................\nEpoch: 1400, accuracy:0.7868,  loss:0.4572,  val_accuracy:0.7623,  val_loss:0.4917,  \n....................................................................................................\nEpoch: 1500, accuracy:0.7621,  loss:0.4860,  val_accuracy:0.7536,  val_loss:0.4856,  \n....................................................................................................\nEpoch: 1600, accuracy:0.7737,  loss:0.4624,  val_accuracy:0.7420,  val_loss:0.4850,  \n....................................................................................................\nEpoch: 1700, accuracy:0.7857,  loss:0.4510,  val_accuracy:0.7246,  val_loss:0.5049,  \n....................................................................................................\nEpoch: 1800, accuracy:0.7817,  loss:0.4492,  val_accuracy:0.7696,  val_loss:0.4647,  \n....................................................................................................\nEpoch: 1900, accuracy:0.7524,  loss:0.5048,  val_accuracy:0.7464,  val_loss:0.4772,  \n....................................................................................................\nFold 1:\n\nEpoch: 0, accuracy:0.4819,  loss:0.7814,  val_accuracy:0.5000,  val_loss:0.7579,  \n....................................................................................................\nEpoch: 100, accuracy:0.6599,  loss:0.6188,  val_accuracy:0.6609,  val_loss:0.6150,  \n....................................................................................................\nEpoch: 200, accuracy:0.6860,  loss:0.5942,  val_accuracy:0.6957,  val_loss:0.5802,  \n....................................................................................................\nEpoch: 300, accuracy:0.6900,  loss:0.5743,  val_accuracy:0.6609,  val_loss:0.6006,  \n....................................................................................................\nEpoch: 400, accuracy:0.7016,  loss:0.5613,  val_accuracy:0.7174,  val_loss:0.5520,  \n....................................................................................................\nEpoch: 500, accuracy:0.7226,  loss:0.5387,  val_accuracy:0.7391,  val_loss:0.5370,  \n....................................................................................................\nEpoch: 600, accuracy:0.7281,  loss:0.5275,  val_accuracy:0.7275,  val_loss:0.5403,  \n....................................................................................................\nEpoch: 700, accuracy:0.7408,  loss:0.5179,  val_accuracy:0.7449,  val_loss:0.5244,  \n....................................................................................................\nEpoch: 800, accuracy:0.7404,  loss:0.5070,  val_accuracy:0.7522,  val_loss:0.5268,  \n....................................................................................................\nEpoch: 900, accuracy:0.7505,  loss:0.5084,  val_accuracy:0.7580,  val_loss:0.5095,  \n....................................................................................................\nEpoch: 1000, accuracy:0.7502,  loss:0.5045,  val_accuracy:0.7087,  val_loss:0.5446,  \n....................................................................................................\nEpoch: 1100, accuracy:0.7165,  loss:0.5448,  val_accuracy:0.7652,  val_loss:0.5109,  \n....................................................................................................\nEpoch: 1200, accuracy:0.7386,  loss:0.5124,  val_accuracy:0.7420,  val_loss:0.5228,  \n....................................................................................................\nEpoch: 1300, accuracy:0.7679,  loss:0.4754,  val_accuracy:0.7696,  val_loss:0.4937,  \n....................................................................................................\nEpoch: 1400, accuracy:0.7451,  loss:0.4951,  val_accuracy:0.7739,  val_loss:0.4920,  \n....................................................................................................\nEpoch: 1500, accuracy:0.7803,  loss:0.4610,  val_accuracy:0.7913,  val_loss:0.4781,  \n....................................................................................................\nEpoch: 1600, accuracy:0.7857,  loss:0.4520,  val_accuracy:0.7899,  val_loss:0.4705,  \n....................................................................................................\nEpoch: 1700, accuracy:0.7806,  loss:0.4511,  val_accuracy:0.7768,  val_loss:0.4811,  \n....................................................................................................\nEpoch: 1800, accuracy:0.7313,  loss:0.5127,  val_accuracy:0.7478,  val_loss:0.5045,  \n....................................................................................................\nEpoch: 1900, accuracy:0.7962,  loss:0.4383,  val_accuracy:0.7913,  val_loss:0.4581,  \n....................................................................................................\nFold 2:\n\nEpoch: 0, accuracy:0.4724,  loss:0.7904,  val_accuracy:0.5000,  val_loss:0.7427,  \n....................................................................................................\nEpoch: 100, accuracy:0.6610,  loss:0.6081,  val_accuracy:0.6580,  val_loss:0.6265,  \n....................................................................................................\nEpoch: 200, accuracy:0.7052,  loss:0.5747,  val_accuracy:0.6884,  val_loss:0.6050,  \n....................................................................................................\nEpoch: 300, accuracy:0.7034,  loss:0.5712,  val_accuracy:0.6739,  val_loss:0.6317,  \n....................................................................................................\nEpoch: 400, accuracy:0.7099,  loss:0.5529,  val_accuracy:0.6986,  val_loss:0.5972,  \n....................................................................................................\nEpoch: 500, accuracy:0.7110,  loss:0.5522,  val_accuracy:0.7101,  val_loss:0.5723,  \n....................................................................................................\nEpoch: 600, accuracy:0.7273,  loss:0.5383,  val_accuracy:0.6957,  val_loss:0.5723,  \n....................................................................................................\nEpoch: 700, accuracy:0.7368,  loss:0.5178,  val_accuracy:0.7043,  val_loss:0.5773,  \n....................................................................................................\nEpoch: 800, accuracy:0.7466,  loss:0.5089,  val_accuracy:0.7188,  val_loss:0.5500,  \n....................................................................................................\nEpoch: 900, accuracy:0.7589,  loss:0.5018,  val_accuracy:0.7203,  val_loss:0.5399,  \n....................................................................................................\nEpoch: 1000, accuracy:0.7498,  loss:0.5041,  val_accuracy:0.7246,  val_loss:0.5420,  \n....................................................................................................\nEpoch: 1100, accuracy:0.7498,  loss:0.5002,  val_accuracy:0.7246,  val_loss:0.5344,  \n....................................................................................................\nEpoch: 1200, accuracy:0.7607,  loss:0.4857,  val_accuracy:0.7362,  val_loss:0.5189,  \n....................................................................................................\nEpoch: 1300, accuracy:0.7418,  loss:0.5204,  val_accuracy:0.7420,  val_loss:0.5158,  \n....................................................................................................\nEpoch: 1400, accuracy:0.7495,  loss:0.4984,  val_accuracy:0.7348,  val_loss:0.5235,  \n....................................................................................................\nEpoch: 1500, accuracy:0.7632,  loss:0.4771,  val_accuracy:0.7493,  val_loss:0.5087,  \n....................................................................................................\nEpoch: 1600, accuracy:0.7806,  loss:0.4645,  val_accuracy:0.7623,  val_loss:0.4863,  \n....................................................................................................\nEpoch: 1700, accuracy:0.7161,  loss:0.5561,  val_accuracy:0.7000,  val_loss:0.5782,  \n....................................................................................................\nEpoch: 1800, accuracy:0.7879,  loss:0.4566,  val_accuracy:0.7594,  val_loss:0.4888,  \n....................................................................................................\nEpoch: 1900, accuracy:0.7973,  loss:0.4456,  val_accuracy:0.7507,  val_loss:0.5055,  \n....................................................................................................\nFold 3:\n\nEpoch: 0, accuracy:0.4839,  loss:0.7824,  val_accuracy:0.5007,  val_loss:0.7118,  \n....................................................................................................\nEpoch: 100, accuracy:0.6549,  loss:0.6249,  val_accuracy:0.6720,  val_loss:0.6153,  \n....................................................................................................\nEpoch: 200, accuracy:0.6796,  loss:0.5972,  val_accuracy:0.6836,  val_loss:0.5846,  \n....................................................................................................\nEpoch: 300, accuracy:0.6970,  loss:0.5768,  val_accuracy:0.6923,  val_loss:0.5728,  \n....................................................................................................\nEpoch: 400, accuracy:0.7010,  loss:0.5695,  val_accuracy:0.7417,  val_loss:0.5383,  \n....................................................................................................\nEpoch: 500, accuracy:0.7068,  loss:0.5523,  val_accuracy:0.7475,  val_loss:0.5231,  \n....................................................................................................\nEpoch: 600, accuracy:0.7249,  loss:0.5344,  val_accuracy:0.7533,  val_loss:0.5101,  \n....................................................................................................\nEpoch: 700, accuracy:0.7340,  loss:0.5314,  val_accuracy:0.7388,  val_loss:0.5127,  \n....................................................................................................\nEpoch: 800, accuracy:0.7082,  loss:0.5589,  val_accuracy:0.7402,  val_loss:0.5101,  \n....................................................................................................\nEpoch: 900, accuracy:0.7470,  loss:0.5109,  val_accuracy:0.7547,  val_loss:0.4902,  \n....................................................................................................\nEpoch: 1000, accuracy:0.7361,  loss:0.5150,  val_accuracy:0.7446,  val_loss:0.5003,  \n....................................................................................................\nEpoch: 1100, accuracy:0.7550,  loss:0.4912,  val_accuracy:0.7576,  val_loss:0.4948,  \n....................................................................................................\nEpoch: 1200, accuracy:0.7361,  loss:0.5083,  val_accuracy:0.7649,  val_loss:0.4848,  \n....................................................................................................\nEpoch: 1300, accuracy:0.7488,  loss:0.4950,  val_accuracy:0.7576,  val_loss:0.4877,  \n....................................................................................................\nEpoch: 1400, accuracy:0.7601,  loss:0.4875,  val_accuracy:0.7808,  val_loss:0.4683,  \n....................................................................................................\nEpoch: 1500, accuracy:0.7760,  loss:0.4711,  val_accuracy:0.7489,  val_loss:0.5041,  \n....................................................................................................\nEpoch: 1600, accuracy:0.7807,  loss:0.4601,  val_accuracy:0.7547,  val_loss:0.5029,  \n....................................................................................................\nEpoch: 1700, accuracy:0.7499,  loss:0.5014,  val_accuracy:0.7358,  val_loss:0.5339,  \n....................................................................................................\nEpoch: 1800, accuracy:0.7938,  loss:0.4431,  val_accuracy:0.7808,  val_loss:0.4482,  \n....................................................................................................\nEpoch: 1900, accuracy:0.7901,  loss:0.4451,  val_accuracy:0.7823,  val_loss:0.4483,  \n....................................................................................................\nFold 4:\n\nEpoch: 0, accuracy:0.4947,  loss:0.7590,  val_accuracy:0.5007,  val_loss:0.7058,  \n....................................................................................................\nEpoch: 100, accuracy:0.6723,  loss:0.6035,  val_accuracy:0.6720,  val_loss:0.6026,  \n....................................................................................................\nEpoch: 200, accuracy:0.7035,  loss:0.5731,  val_accuracy:0.6996,  val_loss:0.5966,  \n....................................................................................................\nEpoch: 300, accuracy:0.7013,  loss:0.5673,  val_accuracy:0.7126,  val_loss:0.5696,  \n....................................................................................................\nEpoch: 400, accuracy:0.7064,  loss:0.5470,  val_accuracy:0.7170,  val_loss:0.5441,  \n....................................................................................................\nEpoch: 500, accuracy:0.7311,  loss:0.5306,  val_accuracy:0.6996,  val_loss:0.5380,  \n....................................................................................................\nEpoch: 600, accuracy:0.7408,  loss:0.5191,  val_accuracy:0.7344,  val_loss:0.5193,  \n....................................................................................................\nEpoch: 700, accuracy:0.7379,  loss:0.5206,  val_accuracy:0.7141,  val_loss:0.5490,  \n....................................................................................................\nEpoch: 800, accuracy:0.7608,  loss:0.4969,  val_accuracy:0.7518,  val_loss:0.5097,  \n....................................................................................................\nEpoch: 900, accuracy:0.7611,  loss:0.4875,  val_accuracy:0.7736,  val_loss:0.4963,  \n....................................................................................................\nEpoch: 1000, accuracy:0.7604,  loss:0.4895,  val_accuracy:0.7678,  val_loss:0.4930,  \n....................................................................................................\nEpoch: 1100, accuracy:0.7597,  loss:0.4930,  val_accuracy:0.7489,  val_loss:0.5004,  \n....................................................................................................\nEpoch: 1200, accuracy:0.7825,  loss:0.4678,  val_accuracy:0.7736,  val_loss:0.4779,  \n....................................................................................................\nEpoch: 1300, accuracy:0.7224,  loss:0.5423,  val_accuracy:0.7736,  val_loss:0.4750,  \n....................................................................................................\nEpoch: 1400, accuracy:0.7829,  loss:0.4602,  val_accuracy:0.7025,  val_loss:0.5634,  \n....................................................................................................\nEpoch: 1500, accuracy:0.7789,  loss:0.4579,  val_accuracy:0.7881,  val_loss:0.4587,  \n....................................................................................................\nEpoch: 1600, accuracy:0.7742,  loss:0.4548,  val_accuracy:0.7170,  val_loss:0.5777,  \n....................................................................................................\nEpoch: 1700, accuracy:0.7789,  loss:0.4552,  val_accuracy:0.7837,  val_loss:0.4566,  \n....................................................................................................\nEpoch: 1800, accuracy:0.8039,  loss:0.4251,  val_accuracy:0.7358,  val_loss:0.5266,  \n....................................................................................................\nEpoch: 1900, accuracy:0.7829,  loss:0.4505,  val_accuracy:0.7983,  val_loss:0.4351,  \n....................................................................................................\n\n\nSource: deep_learning_cv.ipynb\nYou can plot the accuracy vs epochs of each fold and see how they are different for each other\n\n\nplt.plot(range(1, len(histories['fold0'].history['accuracy']) + 1), histories['fold0'].history['accuracy'])\nplt.plot(range(1, len(histories['fold1'].history['accuracy']) + 1), histories['fold1'].history['accuracy'])\nplt.plot(range(1, len(histories['fold1'].history['accuracy']) + 1), histories['fold2'].history['accuracy'])\nplt.plot(range(1, len(histories['fold1'].history['accuracy']) + 1), histories['fold3'].history['accuracy'])\nplt.plot(range(1, len(histories['fold1'].history['accuracy']) + 1), histories['fold4'].history['accuracy'])\n\n\n\n\n\n\nSource: deep_learning_cv.ipynb\n\n\n4.3.3 Plotting the mean accuracy and standard deviation\nLets put together all the results in a dataframe\n\n\nacc_total = []\nfor key in histories.keys():\n    acc_df = pd.DataFrame()\n    acc_df['val_accuracy'] = pd.DataFrame(histories[key].history['val_accuracy'])\n    acc_df['accuracy'] = pd.DataFrame(histories[key].history['accuracy'])\n    acc_df['type'] = key\n    acc_df['epochs'] = range(1, len(histories[key].history['accuracy']) + 1)\n    acc_total.append(acc_df)\n\ndf_total = pd.concat(acc_total)\ndf_total\n\n\n\n\n\n\n\n\nval_accuracy\naccuracy\ntype\nepochs\n\n\n\n\n0\n0.500000\n0.500363\nfold0\n1\n\n\n1\n0.498551\n0.505801\nfold0\n2\n\n\n2\n0.518841\n0.542422\nfold0\n3\n\n\n3\n0.543478\n0.542785\nfold0\n4\n\n\n4\n0.620290\n0.600435\nfold0\n5\n\n\n...\n...\n...\n...\n...\n\n\n1995\n0.780842\n0.794853\nfold4\n1996\n\n\n1996\n0.801161\n0.802102\nfold4\n1997\n\n\n1997\n0.770682\n0.810801\nfold4\n1998\n\n\n1998\n0.799710\n0.793766\nfold4\n1999\n\n\n1999\n0.780842\n0.811163\nfold4\n2000\n\n\n\n\n10000 rows × 4 columns\n\n\n\nSource: deep_learning_cv.ipynb\nand plot a line with the standard deviation using seaborn built-in lineplot function.\n\n\nimport seaborn as sns\n\n\nfig, ax = plt.subplots(figsize=(6,4))\nsns.lineplot(data=df_total, x=\"epochs\", y=\"accuracy\", errorbar=('sd', 1), ax=ax, label='accuracy')\nsns.lineplot(data=df_total, x=\"epochs\", y=\"val_accuracy\", errorbar=('sd', 1), ax=ax, label= 'val accuracy')\nax.set_ylim(0.2, 1)\n\n(0.2, 1.0)\n\n\n\n\n\nSource: deep_learning_cv.ipynb\nYou can expand this even futher by split again the train set into train and validation set. You can also report the accuracy of your model as the mean plus the standard deviation."
  },
  {
    "objectID": "deep_learning.html#tuning-a-deep-learning-model",
    "href": "deep_learning.html#tuning-a-deep-learning-model",
    "title": "4  Deep Learning pipelines",
    "section": "4.4 Tuning a deep learning model",
    "text": "4.4 Tuning a deep learning model\nHyperparameter tuning deep learning models is an art by itself. It will requiere you to run loads of experiements, but also it requieres a deeper understading of your data and model architecture. So, take it easy, it will take time! This example is just a basic pipeline for hyperparameter tuning that once you understand it, you can expand it according to your needs.\nWe are using keras built in tuner (needs to be installed)."
  },
  {
    "objectID": "xgboost.html#xgboost-a-primer",
    "href": "xgboost.html#xgboost-a-primer",
    "title": "5  XGboost for spectroscopy",
    "section": "5.1 XGboost: A primer",
    "text": "5.1 XGboost: A primer\nThis is a simple pipeline to apply if you want to test XGBoost. The new version of XGboost requieres you to encode your tagets from 0 to n-1 classes.\nYou need to install the package xgboost using\npip install xgboost\nif you are using conda enviroment use this instead:\nconda install -c conda-forge py-xgboost\nPlease, check the documentation to learn more.\nWe follow the similar pipeline of load data and extract our features and targets. For plotting purposes, we extract the name of the columns (wavenumbers) and we save them as integers.\ndf = pd.read_csv(\"/Users/mauropazmino/Documents/Learning/Deep_learning_tensor/data/MIRS_temperature_vs_age_20241014.dat\", sep=\"\\t\")\n\n# Extract features and labels\nX = np.asarray(df.iloc[:, 15:-1])\ny = np.asarray(df.iloc[:,0])\n\n# extract the wavenumbers for plots\nwavenumbers = df.iloc[:, 15:-1].columns.astype(int)\n\n5.1.1 Encoding our labels\nTo transform our labels (in this case species) into values of 0 and n-1 classes, we use Label Encoder from sklearn. This process still applies if you have multiple labels.\n\n\n# encoding our labels. XGBoost only accepts values from 0 to n-1 classes\n\nlb = LabelEncoder()\nlb.fit(y)\ny_encoded = lb.transform(y)\ny_encoded\n\narray([0, 0, 0, ..., 1, 1, 1])\n\n\nSource: XGboost.ipynb\n\n\n5.1.2 Importing the model\nWe import the model. You will see all the hyperparameters and their default values.\n\n\nfrom xgboost import XGBClassifier\n\n#define our model\nclf = XGBClassifier()\nclf\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) \n\n\nSource: XGboost.ipynb\n\n\n5.1.3 Train and test\nWe train the model using fit function\n# train\nclf.fit(X_train, y_train)\nWe calculate its accuracy on the test set\n\n\n# predict values\ny_pred = clf.predict(X_test)\n\n# accuracy\nprint(f'Accuracy of XGBoost is: {accuracy_score(y_test, y_pred) * 100:.2f}%')\n\nAccuracy of XGBoost is: 81.96%\n\n\nSource: XGboost.ipynb\nand finally a confusion matrix\nfig, ax = plt.subplots(figsize=(3, 3))\nConfusionMatrixDisplay.from_predictions(lb.inverse_transform(y_test), lb.inverse_transform(y_pred), normalize='true', cmap='Reds',ax=ax)\n\n\n\n5.1.4 Feature importance\nXGBoost have a feature importance attribute that we can use to see which wavenumber has the most influence in the prediction fo species. You can obtain this information with the feature_importances_ attribute. We plot these values agaist the wavenumbers and superimpose a spectra to have a clear idea of which regions of the spectrum are important.\n# feature importance\nfig, ax = plt.subplots(figsize=(8, 3))\nax.plot(wavenumbers, clf.feature_importances_, color='r', label='feature importance')\nax.plot(wavenumbers, X[1], color='k')\nax.set_xlim(4000, 400)\nax.legend()"
  },
  {
    "objectID": "appendix.html#low-intensity",
    "href": "appendix.html#low-intensity",
    "title": "6  A closer look into spectral quality",
    "section": "6.1 Low intensity",
    "text": "6.1 Low intensity\nThe script will measure the spectral intensity by taking the absorbances of each wavelength at 400 to 600 cm\\(^{-1}\\) region and extract the mean. If the value is less than 0.11, the spectral intensity is too low and the sample is discarded.\n\n\n\nFigure 6.2: The mean of the absorbances from 400 to 600 cm-1 is used to assess spectral intensity. If the value is less than 0.11, the sample is discarded\n\n\nHowever, for samples that are to small (sandflies), the value of 0.11 can be decrease to adapt to the overall intensity of those samples."
  },
  {
    "objectID": "appendix.html#atmospheric-intrusion",
    "href": "appendix.html#atmospheric-intrusion",
    "title": "6  A closer look into spectral quality",
    "section": "6.2 Atmospheric intrusion",
    "text": "6.2 Atmospheric intrusion\nWater and CO\\(_2\\) can interfere with the quality of our measurements. It is crucial to follow thorough protocols to measure samples, however, human error is always present (especially samples not dried enough!). To discard samples that have atmospheric intrusion, we use the region from 3500 to 4000 cm\\(^{-1}\\) because water absorbs infrared light in that region. You can check the regions where water and CO\\(_2\\) absorb here\nFirst, we fit a 5th degree polynomial to the region (Figure 6.3). Then, we use R\\(^2\\) to check how well the polynomial fitted the spectra. If there is no water absorption, the value of R\\(^2\\) should be high (close to 1). If there is noise due to water presence, R\\(^2\\) will decrease. If R\\(^2\\) is lower than 0.96, the sample is discarded.\n\n\n\nFigure 6.3: 5th degree polynomial fitted to the region 3500 - 3900 cm\\(^{-1}\\). If R\\(^2\\) is greater than 0.96, the sample is not discarded"
  },
  {
    "objectID": "appendix.html#anvil-distortion",
    "href": "appendix.html#anvil-distortion",
    "title": "6  A closer look into spectral quality",
    "section": "6.3 Anvil distortion",
    "text": "6.3 Anvil distortion"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "González Jiménez, Mario, Simon A. Babayan, Pegah Khazaeli, Margaret\nDoyle, Finlay Walton, Elliott Reedy, Thomas Glew, et al. 2019.\n“Prediction of Mosquito Species and Population Age Structure Using\nMid-Infrared Spectroscopy and Supervised Machine Learning [Version 3;\nPeer Review: 2 Approved].” Wellcome Open Research. https://doi.org/10.12688/wellcomeopenres.15201.3.\n\n\nMwanga, Emmanuel P., Doreen J. Siria, Joshua Mitton, Issa H. Mshani,\nMario González-Jiménez, Prashanth Selvaraj, Klaas Wynne, Francesco\nBaldini, Fredros O. Okumu, and Simon A. Babayan. 2023. “Using\nTransfer Learning and Dimensionality Reduction Techniques to Improve\nGeneralisability of Machine-Learning Predictions of Mosquito Ages from\nMid-Infrared Spectra.” BMC Bioinformatics 24 (1): 11. https://doi.org/10.1186/s12859-022-05128-5.\n\n\nPazmiño-Betancourth, Mauro, Ivan Casas Gómez-Uribarri, Karina\nMondragon-Shem, Simon A Babayan, Francesco Baldini, and Lee Rafuse\nHaines. 2024. “Advancing Age Grading Techniques for\nGlossina Morsitans Morsitans, Vectors of African\nTrypanosomiasis, Through Mid-Infrared Spectroscopy and Machine\nLearning.” Biology Methods and Protocols, August,\nbpae058. https://doi.org/10.1093/biomethods/bpae058.\n\n\nPazmiño-Betancourth, Mauro, Victor Ochoa-Gutiérrez, Heather M. Ferguson,\nMario González-Jiménez, Klaas Wynne, Francesco Baldini, and David\nChilds. 2023. “Evaluation of Diffuse Reflectance Spectroscopy for\nPredicting Age, Species, and Cuticular Resistance of\nAnopheles Gambiae s.l Under Laboratory Conditions.”\nScientific Reports 13 (1): 18499. https://doi.org/10.1038/s41598-023-45696-x.\n\n\nSiria, Doreen J., Roger Sanou, Joshua Mitton, Emmanuel P. Mwanga,\nAbdoulaye Niang, Issiaka Sare, Paul C. D. Johnson, et al. 2022.\n“Rapid Age-Grading and Species Identification of Natural\nMosquitoes for Malaria Surveillance.” Nature\nCommunications 13 (1): 1–9. https://doi.org/10.1038/s41467-022-28980-8.\n\n\nVabalas, Andrius, Emma Gowen, Ellen Poliakoff, and Alexander J. Casson.\n2019. “Machine Learning Algorithm Validation with a Limited Sample\nSize.” PLoS ONE 14 (11): e0224365. https://doi.org/10.1371/journal.pone.0224365.\n\n\nVarma, Sudhir, and Richard Simon. 2006. “Bias in Error Estimation\nWhen Using Cross-Validation for Model Selection.” BMC\nBioinformatics 7 (1). https://doi.org/10.1186/1471-2105-7-91."
  }
]